{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMQbYZNVGqLm9NyMr1wYt1v"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"41e119fc46a44f1c9dee1d4435658950":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_202b360550a54ec0b40cc57d3b92718b","IPY_MODEL_748021f1c28f469fbb82f94c88cd30cb","IPY_MODEL_b8f8d1fffa684f50aa6e554d3ceb9bfb"],"layout":"IPY_MODEL_2836d2d9585c46799b66e7b413e094e7"}},"202b360550a54ec0b40cc57d3b92718b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d40c3d06f75440f1beb79c5d784435c5","placeholder":"​","style":"IPY_MODEL_e31a71ba8d3341c08700faf671328722","value":"config.json: 100%"}},"748021f1c28f469fbb82f94c88cd30cb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eaa155a97176432ca2b52e866338a856","max":625,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d3aa52ca635f4227b873fa159af5a3c5","value":625}},"b8f8d1fffa684f50aa6e554d3ceb9bfb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_124e84dbd3b045db84a459c68d7d7000","placeholder":"​","style":"IPY_MODEL_aab3a951cf89466f97f22d8201ed6e37","value":" 625/625 [00:00&lt;00:00, 39.6kB/s]"}},"2836d2d9585c46799b66e7b413e094e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d40c3d06f75440f1beb79c5d784435c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e31a71ba8d3341c08700faf671328722":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eaa155a97176432ca2b52e866338a856":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3aa52ca635f4227b873fa159af5a3c5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"124e84dbd3b045db84a459c68d7d7000":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aab3a951cf89466f97f22d8201ed6e37":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"474167ec9f804f31b829d3f6f275c75b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_94950f1a888f4b5f949ff924125d5fcd","IPY_MODEL_c58a4be62e4440028c3aa91017648b9b","IPY_MODEL_9d1b81a61be24c28930fcf6199507d7d"],"layout":"IPY_MODEL_ad5362e29ab745d7939b7efd7479dab0"}},"94950f1a888f4b5f949ff924125d5fcd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a94fd7bed1014dd8ab8930e84db06e24","placeholder":"​","style":"IPY_MODEL_b92037f7295d42bebb7cddf610c9aba6","value":"tokenizer_config.json: 100%"}},"c58a4be62e4440028c3aa91017648b9b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b923b5f56a4d49dfbfca6b84b36c799a","max":29,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a210fec3b3484589b7cd1793f06125d3","value":29}},"9d1b81a61be24c28930fcf6199507d7d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ec3b4e8d634b4593999b1d4ccd4626af","placeholder":"​","style":"IPY_MODEL_da674ec5e7754911aa9ddd9b74f8fda2","value":" 29.0/29.0 [00:00&lt;00:00, 1.95kB/s]"}},"ad5362e29ab745d7939b7efd7479dab0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a94fd7bed1014dd8ab8930e84db06e24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b92037f7295d42bebb7cddf610c9aba6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b923b5f56a4d49dfbfca6b84b36c799a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a210fec3b3484589b7cd1793f06125d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ec3b4e8d634b4593999b1d4ccd4626af":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da674ec5e7754911aa9ddd9b74f8fda2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f5fa515d5b794d319dba563b2d86995d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_33c0b0efc56940dcb5b6a4339199c489","IPY_MODEL_023ca90d48994cfa8b5d5b0ce66776c7","IPY_MODEL_58ab23e652d742139e2b304905c86faf"],"layout":"IPY_MODEL_01c5c7019fad482ea1cf1cbb878f7626"}},"33c0b0efc56940dcb5b6a4339199c489":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_144a86dbf7f94bfd969edbbde0cac55e","placeholder":"​","style":"IPY_MODEL_552e8bf517e349b99474e9a812306956","value":"vocab.txt: 100%"}},"023ca90d48994cfa8b5d5b0ce66776c7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a46f89f0e90f4f87afcaf8ca43b66603","max":995526,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ac33d7327b74281b68a6dcad31fb7d2","value":995526}},"58ab23e652d742139e2b304905c86faf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f21e3ab19af9455e9052185db814452c","placeholder":"​","style":"IPY_MODEL_3472d917ebde422f906dbcadaf2ef7cc","value":" 996k/996k [00:00&lt;00:00, 3.83MB/s]"}},"01c5c7019fad482ea1cf1cbb878f7626":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"144a86dbf7f94bfd969edbbde0cac55e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"552e8bf517e349b99474e9a812306956":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a46f89f0e90f4f87afcaf8ca43b66603":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ac33d7327b74281b68a6dcad31fb7d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f21e3ab19af9455e9052185db814452c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3472d917ebde422f906dbcadaf2ef7cc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0209a91c0a874808aa234633d9399e72":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b56238605a284bc18a91257f112914e2","IPY_MODEL_01f1aae1b98b425cb0661ad1092ff350","IPY_MODEL_7df45ffc3c0b498baa3f65abb932a109"],"layout":"IPY_MODEL_15c5afdc87154de2bb485c8706e3931a"}},"b56238605a284bc18a91257f112914e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10cdf9bf4f1048c282fdc0ea5e9426e6","placeholder":"​","style":"IPY_MODEL_91b6cef80b7943c8a8be259757779e2b","value":"tokenizer.json: 100%"}},"01f1aae1b98b425cb0661ad1092ff350":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_66a78613a4f540849d376e9335016885","max":1961828,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d723f97cb4d8467bb23574f9b5879939","value":1961828}},"7df45ffc3c0b498baa3f65abb932a109":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2f07d9271824fa6bcec9c481865e41c","placeholder":"​","style":"IPY_MODEL_7e981c0f304c42ef91c4d1a9e77f53d7","value":" 1.96M/1.96M [00:00&lt;00:00, 7.76MB/s]"}},"15c5afdc87154de2bb485c8706e3931a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10cdf9bf4f1048c282fdc0ea5e9426e6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91b6cef80b7943c8a8be259757779e2b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66a78613a4f540849d376e9335016885":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d723f97cb4d8467bb23574f9b5879939":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f2f07d9271824fa6bcec9c481865e41c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e981c0f304c42ef91c4d1a9e77f53d7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"43656fa1eb4346c582d2f8845d8a81c9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_83ae1ac556f643d7972fed2e1dcf8c2e","IPY_MODEL_da94720645a3472499f9a5ff2cd29020","IPY_MODEL_2e62f55051684830bc4a27b754456213"],"layout":"IPY_MODEL_e75729715eda4493bbe62494af8a29fb"}},"83ae1ac556f643d7972fed2e1dcf8c2e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_da4428afdf824430acd7727c7be3a58e","placeholder":"​","style":"IPY_MODEL_2c2d2120bafa4041ad110587d592bdab","value":"model.safetensors: 100%"}},"da94720645a3472499f9a5ff2cd29020":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ffe5f808fdd44bb94af72de77615e2c","max":714290682,"min":0,"orientation":"horizontal","style":"IPY_MODEL_590b8efcfedb40c0a33ea508983ba60e","value":714290682}},"2e62f55051684830bc4a27b754456213":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8cdea7318264995a7a582b3e14ddfc5","placeholder":"​","style":"IPY_MODEL_054ec626678a4558a1ccee0289279306","value":" 714M/714M [00:04&lt;00:00, 163MB/s]"}},"e75729715eda4493bbe62494af8a29fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"da4428afdf824430acd7727c7be3a58e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c2d2120bafa4041ad110587d592bdab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ffe5f808fdd44bb94af72de77615e2c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"590b8efcfedb40c0a33ea508983ba60e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8cdea7318264995a7a582b3e14ddfc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"054ec626678a4558a1ccee0289279306":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"TRl0bnAN4C9-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1707419076657,"user_tz":-420,"elapsed":20357,"user":{"displayName":"Toàn Nguyễn Văn","userId":"12622235312397842785"}},"outputId":"d636af5f-78e1-4cab-9e55-d423084765e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["%cd '/content/drive/MyDrive/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bZIjFGFD4RVH","executionInfo":{"status":"ok","timestamp":1707419076657,"user_tz":-420,"elapsed":12,"user":{"displayName":"Toàn Nguyễn Văn","userId":"12622235312397842785"}},"outputId":"d91ba13d-cc47-4ce3-98fb-3678229bc60b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["\n","%cd '/content/drive/MyDrive/Bert'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xabn97lNEU5b","executionInfo":{"status":"ok","timestamp":1707419076657,"user_tz":-420,"elapsed":9,"user":{"displayName":"Toàn Nguyễn Văn","userId":"12622235312397842785"}},"outputId":"3eba8ba8-af46-4634-c269-cc2098e3f373"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Bert\n"]}]},{"cell_type":"code","source":["!pip install transformers\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ERV8jC51EV7f","executionInfo":{"status":"ok","timestamp":1707419081116,"user_tz":-420,"elapsed":4467,"user":{"displayName":"Toàn Nguyễn Văn","userId":"12622235312397842785"}},"outputId":"429ad7e2-c93c-4977-83d4-fca1fd0627a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"]}]},{"cell_type":"code","source":["import json, collections, os, random, glob, math, string, re, torch\n","import numpy as np\n","import timeit\n","from tqdm import trange, tqdm_notebook as tqdm\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from transformers import WEIGHTS_NAME, BertConfig, BertForQuestionAnswering, BertTokenizerFast, BasicTokenizer, AdamW, get_linear_schedule_with_warmup\n","# from transformers.tokenization_bert import whitespace_tokenize\n","from nltk.tokenize import WhitespaceTokenizer\n","from transformers.data.processors.squad import SquadResult, SquadV1Processor, SquadV2Processor"],"metadata":{"id":"w2t3prgc4WuB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import logging\n","import math\n","import re\n","import string\n","\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","def normalize_answer(s):\n","    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n","    def white_space_fix(text):\n","        return \" \".join(text.split())\n","\n","    def remove_punc(text):\n","        exclude = set(string.punctuation)\n","        return \"\".join(ch for ch in text if ch not in exclude)\n","\n","    def lower(text):\n","        return text.lower()\n","\n","    return white_space_fix(remove_punc(lower(s)))\n","\n","\n","def get_tokens(s):\n","    if not s:\n","        return []\n","    return normalize_answer(s).split()\n","\n","\n","def compute_exact(a_gold, a_pred):\n","    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n","\n","\n","def compute_f1(a_gold, a_pred):\n","    gold_toks = get_tokens(a_gold)\n","    pred_toks = get_tokens(a_pred)\n","    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n","    num_same = sum(common.values())\n","    if len(gold_toks) == 0 or len(pred_toks) == 0:\n","        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n","        return int(gold_toks == pred_toks)\n","    if num_same == 0:\n","        return 0\n","    precision = 1.0 * num_same / len(pred_toks)\n","    recall = 1.0 * num_same / len(gold_toks)\n","    f1 = (2 * precision * recall) / (precision + recall)\n","    return f1\n","\n","\n","def get_raw_scores(examples, preds):\n","    \"\"\"\n","    Computes the exact and f1 scores from the examples and the model predictions\n","    \"\"\"\n","    exact_scores = {}\n","    f1_scores = {}\n","\n","    for example in examples:\n","        qas_id = example.qas_id\n","        gold_answers = [answer[\"text\"] for answer in example.answers if normalize_answer(answer[\"text\"])]\n","\n","        if not gold_answers:\n","            # For unanswerable questions, only correct answer is empty string\n","            gold_answers = [\"\"]\n","\n","        if qas_id not in preds:\n","            print(\"Missing prediction for %s\" % qas_id)\n","            continue\n","\n","        prediction = preds[qas_id]\n","        exact_scores[qas_id] = max(compute_exact(a, prediction) for a in gold_answers)\n","        f1_scores[qas_id] = max(compute_f1(a, prediction) for a in gold_answers)\n","\n","    return exact_scores, f1_scores\n","\n","\n","def apply_no_ans_threshold(scores, na_probs, qid_to_has_ans, na_prob_thresh):\n","    new_scores = {}\n","    for qid, s in scores.items():\n","        pred_na = na_probs[qid] > na_prob_thresh\n","        if pred_na:\n","            new_scores[qid] = float(not qid_to_has_ans[qid])\n","        else:\n","            new_scores[qid] = s\n","    return new_scores\n","\n","\n","def make_eval_dict(exact_scores, f1_scores, qid_list=None):\n","    if not qid_list:\n","        total = len(exact_scores)\n","        return collections.OrderedDict(\n","            [\n","                (\"exact\", 100.0 * sum(exact_scores.values()) / total),\n","                (\"f1\", 100.0 * sum(f1_scores.values()) / total),\n","                (\"total\", total),\n","            ]\n","        )\n","    else:\n","        total = len(qid_list)\n","        return collections.OrderedDict(\n","            [\n","                (\"exact\", 100.0 * sum(exact_scores[k] for k in qid_list) / total),\n","                (\"f1\", 100.0 * sum(f1_scores[k] for k in qid_list) / total),\n","                (\"total\", total),\n","            ]\n","        )\n","\n","\n","def merge_eval(main_eval, new_eval, prefix):\n","    for k in new_eval:\n","        main_eval[\"%s_%s\" % (prefix, k)] = new_eval[k]\n","\n","\n","def find_best_thresh_v2(preds, scores, na_probs, qid_to_has_ans):\n","    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n","    cur_score = num_no_ans\n","    best_score = cur_score\n","    best_thresh = 0.0\n","    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n","    for i, qid in enumerate(qid_list):\n","        if qid not in scores:\n","            continue\n","        if qid_to_has_ans[qid]:\n","            diff = scores[qid]\n","        else:\n","            if preds[qid]:\n","                diff = -1\n","            else:\n","                diff = 0\n","        cur_score += diff\n","        if cur_score > best_score:\n","            best_score = cur_score\n","            best_thresh = na_probs[qid]\n","\n","    has_ans_score, has_ans_cnt = 0, 0\n","    for qid in qid_list:\n","        if not qid_to_has_ans[qid]:\n","            continue\n","        has_ans_cnt += 1\n","\n","        if qid not in scores:\n","            continue\n","        has_ans_score += scores[qid]\n","\n","    return 100.0 * best_score / len(scores), best_thresh, 1.0 * has_ans_score / has_ans_cnt\n","\n","\n","def find_all_best_thresh_v2(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n","    best_exact, exact_thresh, has_ans_exact = find_best_thresh_v2(preds, exact_raw, na_probs, qid_to_has_ans)\n","    best_f1, f1_thresh, has_ans_f1 = find_best_thresh_v2(preds, f1_raw, na_probs, qid_to_has_ans)\n","    main_eval[\"best_exact\"] = best_exact\n","    main_eval[\"best_exact_thresh\"] = exact_thresh\n","    main_eval[\"best_f1\"] = best_f1\n","    main_eval[\"best_f1_thresh\"] = f1_thresh\n","    main_eval[\"has_ans_exact\"] = has_ans_exact\n","    main_eval[\"has_ans_f1\"] = has_ans_f1\n","\n","\n","def find_best_thresh(preds, scores, na_probs, qid_to_has_ans):\n","    num_no_ans = sum(1 for k in qid_to_has_ans if not qid_to_has_ans[k])\n","    cur_score = num_no_ans\n","    best_score = cur_score\n","    best_thresh = 0.0\n","    qid_list = sorted(na_probs, key=lambda k: na_probs[k])\n","    for _, qid in enumerate(qid_list):\n","        if qid not in scores:\n","            continue\n","        if qid_to_has_ans[qid]:\n","            diff = scores[qid]\n","        else:\n","            if preds[qid]:\n","                diff = -1\n","            else:\n","                diff = 0\n","        cur_score += diff\n","        if cur_score > best_score:\n","            best_score = cur_score\n","            best_thresh = na_probs[qid]\n","    return 100.0 * best_score / len(scores), best_thresh\n","\n","\n","def find_all_best_thresh(main_eval, preds, exact_raw, f1_raw, na_probs, qid_to_has_ans):\n","    best_exact, exact_thresh = find_best_thresh(preds, exact_raw, na_probs, qid_to_has_ans)\n","    best_f1, f1_thresh = find_best_thresh(preds, f1_raw, na_probs, qid_to_has_ans)\n","\n","    main_eval[\"best_exact\"] = best_exact\n","    main_eval[\"best_exact_thresh\"] = exact_thresh\n","    main_eval[\"best_f1\"] = best_f1\n","    main_eval[\"best_f1_thresh\"] = f1_thresh\n","\n","\n","def squad_evaluate(examples, preds, no_answer_probs=None, no_answer_probability_threshold=1.0):\n","    qas_id_to_has_answer = {example.qas_id: bool(example.answers) for example in examples}\n","    has_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if has_answer]\n","    no_answer_qids = [qas_id for qas_id, has_answer in qas_id_to_has_answer.items() if not has_answer]\n","\n","    if no_answer_probs is None:\n","        no_answer_probs = {k: 0.0 for k in preds}\n","\n","    exact, f1 = get_raw_scores(examples, preds)\n","\n","    exact_threshold = apply_no_ans_threshold(\n","        exact, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold\n","    )\n","    f1_threshold = apply_no_ans_threshold(f1, no_answer_probs, qas_id_to_has_answer, no_answer_probability_threshold)\n","\n","    evaluation = make_eval_dict(exact_threshold, f1_threshold)\n","\n","    if has_answer_qids:\n","        has_ans_eval = make_eval_dict(exact_threshold, f1_threshold, qid_list=has_answer_qids)\n","        merge_eval(evaluation, has_ans_eval, \"HasAns\")\n","\n","    if no_answer_qids:\n","        no_ans_eval = make_eval_dict(exact_threshold, f1_threshold, qid_list=no_answer_qids)\n","        merge_eval(evaluation, no_ans_eval, \"NoAns\")\n","\n","    if no_answer_probs:\n","        find_all_best_thresh(evaluation, preds, exact, f1, no_answer_probs, qas_id_to_has_answer)\n","\n","    return evaluation\n","\n","\n","def get_final_text(pred_text, orig_text, do_lower_case, verbose_logging=False):\n","    \"\"\"Project the tokenized prediction back to the original text.\"\"\"\n","\n","    # When we created the data, we kept track of the alignment between original\n","    # (whitespace tokenized) tokens and our WordPiece tokenized tokens. So\n","    # now `orig_text` contains the span of our original text corresponding to the\n","    # span that we predicted.\n","    #\n","    # However, `orig_text` may contain extra characters that we don't want in\n","    # our prediction.\n","    #\n","    # For example, let's say:\n","    #   pred_text = steve smith\n","    #   orig_text = Steve Smith's\n","    #\n","    # We don't want to return `orig_text` because it contains the extra \"'s\".\n","    #\n","    # We don't want to return `pred_text` because it's already been normalized\n","    # (the SQuAD eval script also does punctuation stripping/lower casing but\n","    # our tokenizer does additional normalization like stripping accent\n","    # characters).\n","    #\n","    # What we really want to return is \"Steve Smith\".\n","    #\n","    # Therefore, we have to apply a semi-complicated alignment heuristic between\n","    # `pred_text` and `orig_text` to get a character-to-character alignment. This\n","    # can fail in certain cases in which case we just return `orig_text`.\n","\n","    def _strip_spaces(text):\n","        ns_chars = []\n","        ns_to_s_map = collections.OrderedDict()\n","        for (i, c) in enumerate(text):\n","            if c == \" \":\n","                continue\n","            ns_to_s_map[len(ns_chars)] = i\n","            ns_chars.append(c)\n","        ns_text = \"\".join(ns_chars)\n","        return (ns_text, ns_to_s_map)\n","\n","    # We first tokenize `orig_text`, strip whitespace from the result\n","    # and `pred_text`, and check if they are the same length. If they are\n","    # NOT the same length, the heuristic has failed. If they are the same\n","    # length, we assume the characters are one-to-one aligned.\n","    tokenizer = BasicTokenizer(do_lower_case=do_lower_case)\n","\n","    tok_text = \" \".join(tokenizer.tokenize(orig_text))\n","\n","    start_position = tok_text.find(pred_text)\n","    if start_position == -1:\n","        if verbose_logging:\n","            logger.info(\"Unable to find text: '%s' in '%s'\" % (pred_text, orig_text))\n","        return orig_text\n","    end_position = start_position + len(pred_text) - 1\n","\n","    (orig_ns_text, orig_ns_to_s_map) = _strip_spaces(orig_text)\n","    (tok_ns_text, tok_ns_to_s_map) = _strip_spaces(tok_text)\n","\n","    if len(orig_ns_text) != len(tok_ns_text):\n","        if verbose_logging:\n","            logger.info(\"Length not equal after stripping spaces: '%s' vs '%s'\", orig_ns_text, tok_ns_text)\n","        return orig_text\n","\n","    # We then project the characters in `pred_text` back to `orig_text` using\n","    # the character-to-character alignment.\n","    tok_s_to_ns_map = {}\n","    for (i, tok_index) in tok_ns_to_s_map.items():\n","        tok_s_to_ns_map[tok_index] = i\n","\n","    orig_start_position = None\n","    if start_position in tok_s_to_ns_map:\n","        ns_start_position = tok_s_to_ns_map[start_position]\n","        if ns_start_position in orig_ns_to_s_map:\n","            orig_start_position = orig_ns_to_s_map[ns_start_position]\n","\n","    if orig_start_position is None:\n","        if verbose_logging:\n","            logger.info(\"Couldn't map start position\")\n","        return orig_text\n","\n","    orig_end_position = None\n","    if end_position in tok_s_to_ns_map:\n","        ns_end_position = tok_s_to_ns_map[end_position]\n","        if ns_end_position in orig_ns_to_s_map:\n","            orig_end_position = orig_ns_to_s_map[ns_end_position]\n","\n","    if orig_end_position is None:\n","        if verbose_logging:\n","            logger.info(\"Couldn't map end position\")\n","        return orig_text\n","\n","    output_text = orig_text[orig_start_position : (orig_end_position + 1)]\n","    return output_text\n","\n","\n","def _get_best_indexes(logits, n_best_size):\n","    \"\"\"Get the n-best logits from a list.\"\"\"\n","    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n","\n","    best_indexes = []\n","    for i in range(len(index_and_score)):\n","        if i >= n_best_size:\n","            break\n","        best_indexes.append(index_and_score[i][0])\n","    return best_indexes\n","\n","\n","def _compute_softmax(scores):\n","    \"\"\"Compute softmax probability over raw logits.\"\"\"\n","    if not scores:\n","        return []\n","\n","    max_score = None\n","    for score in scores:\n","        if max_score is None or score > max_score:\n","            max_score = score\n","\n","    exp_scores = []\n","    total_sum = 0.0\n","    for score in scores:\n","        x = math.exp(score - max_score)\n","        exp_scores.append(x)\n","        total_sum += x\n","\n","    probs = []\n","    for score in exp_scores:\n","        probs.append(score / total_sum)\n","    return probs\n","\n","\n","def compute_predictions_logits(\n","    all_examples,\n","    all_features,\n","    all_results,\n","    n_best_size,\n","    max_answer_length,\n","    do_lower_case,\n","    output_prediction_file,\n","    output_nbest_file,\n","    output_null_log_odds_file,\n","    verbose_logging,\n","    version_2_with_negative,\n","    null_score_diff_threshold,\n","    tokenizer,\n","):\n","    \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n","    logger.info(\"Writing predictions to: %s\" % (output_prediction_file))\n","    logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n","\n","    example_index_to_features = collections.defaultdict(list)\n","    for feature in all_features:\n","        example_index_to_features[feature.example_index].append(feature)\n","\n","    unique_id_to_result = {}\n","    for result in all_results:\n","        unique_id_to_result[result.unique_id] = result\n","\n","    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","        \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"]\n","    )\n","\n","    all_predictions = collections.OrderedDict()\n","    all_nbest_json = collections.OrderedDict()\n","    scores_diff_json = collections.OrderedDict()\n","\n","    for (example_index, example) in enumerate(all_examples):\n","        features = example_index_to_features[example_index]\n","\n","        prelim_predictions = []\n","        # keep track of the minimum score of null start+end of position 0\n","        score_null = 1000000  # large and positive\n","        min_null_feature_index = 0  # the paragraph slice with min null score\n","        null_start_logit = 0  # the start logit at the slice with min null score\n","        null_end_logit = 0  # the end logit at the slice with min null score\n","        for (feature_index, feature) in enumerate(features):\n","            result = unique_id_to_result[feature.unique_id]\n","            start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n","            end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n","            # if we could have irrelevant answers, get the min score of irrelevant\n","            if version_2_with_negative:\n","                feature_null_score = result.start_logits[0] + result.end_logits[0]\n","                if feature_null_score < score_null:\n","                    score_null = feature_null_score\n","                    min_null_feature_index = feature_index\n","                    null_start_logit = result.start_logits[0]\n","                    null_end_logit = result.end_logits[0]\n","            for start_index in start_indexes:\n","                for end_index in end_indexes:\n","                    # We could hypothetically create invalid predictions, e.g., predict\n","                    # that the start of the span is in the question. We throw out all\n","                    # invalid predictions.\n","                    if start_index >= len(feature.tokens):\n","                        continue\n","                    if end_index >= len(feature.tokens):\n","                        continue\n","                    if start_index not in feature.token_to_orig_map:\n","                        continue\n","                    if end_index not in feature.token_to_orig_map:\n","                        continue\n","                    if not feature.token_is_max_context.get(start_index, False):\n","                        continue\n","                    if end_index < start_index:\n","                        continue\n","                    length = end_index - start_index + 1\n","                    if length > max_answer_length:\n","                        continue\n","                    prelim_predictions.append(\n","                        _PrelimPrediction(\n","                            feature_index=feature_index,\n","                            start_index=start_index,\n","                            end_index=end_index,\n","                            start_logit=result.start_logits[start_index],\n","                            end_logit=result.end_logits[end_index],\n","                        )\n","                    )\n","        if version_2_with_negative:\n","            prelim_predictions.append(\n","                _PrelimPrediction(\n","                    feature_index=min_null_feature_index,\n","                    start_index=0,\n","                    end_index=0,\n","                    start_logit=null_start_logit,\n","                    end_logit=null_end_logit,\n","                )\n","            )\n","        prelim_predictions = sorted(prelim_predictions, key=lambda x: (x.start_logit + x.end_logit), reverse=True)\n","\n","        _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","            \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"]\n","        )\n","\n","        seen_predictions = {}\n","        nbest = []\n","        for pred in prelim_predictions:\n","            if len(nbest) >= n_best_size:\n","                break\n","            feature = features[pred.feature_index]\n","            if pred.start_index > 0:  # this is a non-null prediction\n","                tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n","                orig_doc_start = feature.token_to_orig_map[pred.start_index]\n","                orig_doc_end = feature.token_to_orig_map[pred.end_index]\n","                orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n","                #print(tok_tokens)\n","                #tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n","                tok_text = tok_tokens\n","                tok_text = \" \".join(tok_tokens)\n","                #\n","                # # De-tokenize WordPieces that have been split off.\n","                # tok_text = tok_text.replace(\" ##\", \"\")\n","                # tok_text = tok_text.replace(\"##\", \"\")\n","\n","                # Clean whitespace\n","                tok_text = tok_text.strip()\n","                tok_text = \" \".join(tok_text.split())\n","                orig_text = \" \".join(orig_tokens)\n","\n","                final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n","                if final_text in seen_predictions:\n","                    continue\n","\n","                seen_predictions[final_text] = True\n","            else:\n","                final_text = \"\"\n","                seen_predictions[final_text] = True\n","\n","            nbest.append(_NbestPrediction(text=final_text, start_logit=pred.start_logit, end_logit=pred.end_logit))\n","        # if we didn't include the empty option in the n-best, include it\n","        if version_2_with_negative:\n","            if \"\" not in seen_predictions:\n","                nbest.append(_NbestPrediction(text=\"\", start_logit=null_start_logit, end_logit=null_end_logit))\n","\n","            # In very rare edge cases we could only have single null prediction.\n","            # So we just create a nonce prediction in this case to avoid failure.\n","            if len(nbest) == 1:\n","                nbest.insert(0, _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n","\n","        # In very rare edge cases we could have no valid predictions. So we\n","        # just create a nonce prediction in this case to avoid failure.\n","        if not nbest:\n","            nbest.append(_NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n","\n","        assert len(nbest) >= 1\n","\n","        total_scores = []\n","        best_non_null_entry = None\n","        for entry in nbest:\n","            total_scores.append(entry.start_logit + entry.end_logit)\n","            if not best_non_null_entry:\n","                if entry.text:\n","                    best_non_null_entry = entry\n","\n","        probs = _compute_softmax(total_scores)\n","\n","        nbest_json = []\n","        for (i, entry) in enumerate(nbest):\n","            output = collections.OrderedDict()\n","            output[\"text\"] = entry.text\n","            output[\"probability\"] = probs[i]\n","            output[\"start_logit\"] = entry.start_logit\n","            output[\"end_logit\"] = entry.end_logit\n","            nbest_json.append(output)\n","\n","        assert len(nbest_json) >= 1\n","\n","        if not version_2_with_negative:\n","            all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n","        else:\n","            # predict \"\" iff the null score - the score of best non-null > threshold\n","            score_diff = score_null - best_non_null_entry.start_logit - (best_non_null_entry.end_logit)\n","            scores_diff_json[example.qas_id] = score_diff\n","            if score_diff > null_score_diff_threshold:\n","                all_predictions[example.qas_id] = \"\"\n","            else:\n","                all_predictions[example.qas_id] = best_non_null_entry.text\n","        all_nbest_json[example.qas_id] = nbest_json\n","\n","    with open(output_prediction_file, \"w\") as writer:\n","        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n","\n","    with open(output_nbest_file, \"w\") as writer:\n","        writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n","\n","    if version_2_with_negative:\n","        with open(output_null_log_odds_file, \"w\") as writer:\n","            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n","\n","    return all_predictions\n","\n","\n","def compute_predictions_log_probs(\n","    all_examples,\n","    all_features,\n","    all_results,\n","    n_best_size,\n","    max_answer_length,\n","    output_prediction_file,\n","    output_nbest_file,\n","    output_null_log_odds_file,\n","    start_n_top,\n","    end_n_top,\n","    version_2_with_negative,\n","    tokenizer,\n","    verbose_logging,\n","):\n","    \"\"\" XLNet write prediction logic (more complex than Bert's).\n","        Write final predictions to the json file and log-odds of null if needed.\n","\n","        Requires utils_squad_evaluate.py\n","    \"\"\"\n","    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","        \"PrelimPrediction\", [\"feature_index\", \"start_index\", \"end_index\", \"start_log_prob\", \"end_log_prob\"]\n","    )\n","\n","    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n","        \"NbestPrediction\", [\"text\", \"start_log_prob\", \"end_log_prob\"]\n","    )\n","\n","    logger.info(\"Writing predictions to: %s\", output_prediction_file)\n","    # logger.info(\"Writing nbest to: %s\" % (output_nbest_file))\n","\n","    example_index_to_features = collections.defaultdict(list)\n","    for feature in all_features:\n","        example_index_to_features[feature.example_index].append(feature)\n","\n","    unique_id_to_result = {}\n","    for result in all_results:\n","        unique_id_to_result[result.unique_id] = result\n","\n","    all_predictions = collections.OrderedDict()\n","    all_nbest_json = collections.OrderedDict()\n","    scores_diff_json = collections.OrderedDict()\n","\n","    for (example_index, example) in enumerate(all_examples):\n","        features = example_index_to_features[example_index]\n","\n","        prelim_predictions = []\n","        # keep track of the minimum score of null start+end of position 0\n","        score_null = 1000000  # large and positive\n","\n","        for (feature_index, feature) in enumerate(features):\n","            result = unique_id_to_result[feature.unique_id]\n","\n","            cur_null_score = result.cls_logits\n","\n","            # if we could have irrelevant answers, get the min score of irrelevant\n","            score_null = min(score_null, cur_null_score)\n","\n","            for i in range(start_n_top):\n","                for j in range(end_n_top):\n","                    start_log_prob = result.start_logits[i]\n","                    start_index = result.start_top_index[i]\n","\n","                    j_index = i * end_n_top + j\n","\n","                    end_log_prob = result.end_logits[j_index]\n","                    end_index = result.end_top_index[j_index]\n","\n","                    # We could hypothetically create invalid predictions, e.g., predict\n","                    # that the start of the span is in the question. We throw out all\n","                    # invalid predictions.\n","                    if start_index >= feature.paragraph_len - 1:\n","                        continue\n","                    if end_index >= feature.paragraph_len - 1:\n","                        continue\n","\n","                    if not feature.token_is_max_context.get(start_index, False):\n","                        continue\n","                    if end_index < start_index:\n","                        continue\n","                    length = end_index - start_index + 1\n","                    if length > max_answer_length:\n","                        continue\n","\n","                    prelim_predictions.append(\n","                        _PrelimPrediction(\n","                            feature_index=feature_index,\n","                            start_index=start_index,\n","                            end_index=end_index,\n","                            start_log_prob=start_log_prob,\n","                            end_log_prob=end_log_prob,\n","                        )\n","                    )\n","\n","        prelim_predictions = sorted(\n","            prelim_predictions, key=lambda x: (x.start_log_prob + x.end_log_prob), reverse=True\n","        )\n","\n","        seen_predictions = {}\n","        nbest = []\n","        for pred in prelim_predictions:\n","            if len(nbest) >= n_best_size:\n","                break\n","            feature = features[pred.feature_index]\n","\n","            # XLNet un-tokenizer\n","            # Let's keep it simple for now and see if we need all this later.\n","            #\n","            # tok_start_to_orig_index = feature.tok_start_to_orig_index\n","            # tok_end_to_orig_index = feature.tok_end_to_orig_index\n","            # start_orig_pos = tok_start_to_orig_index[pred.start_index]\n","            # end_orig_pos = tok_end_to_orig_index[pred.end_index]\n","            # paragraph_text = example.paragraph_text\n","            # final_text = paragraph_text[start_orig_pos: end_orig_pos + 1].strip()\n","\n","            # Previously used Bert untokenizer\n","            tok_tokens = feature.tokens[pred.start_index : (pred.end_index + 1)]\n","            orig_doc_start = feature.token_to_orig_map[pred.start_index]\n","            orig_doc_end = feature.token_to_orig_map[pred.end_index]\n","            orig_tokens = example.doc_tokens[orig_doc_start : (orig_doc_end + 1)]\n","            tok_text = tokenizer.convert_tokens_to_string(tok_tokens)\n","\n","            # Clean whitespace\n","            tok_text = tok_text.strip()\n","            tok_text = \" \".join(tok_text.split())\n","            orig_text = \" \".join(orig_tokens)\n","\n","            if hasattr(tokenizer, \"do_lower_case\"):\n","                do_lower_case = tokenizer.do_lower_case\n","            else:\n","                do_lower_case = tokenizer.do_lowercase_and_remove_accent\n","\n","            final_text = get_final_text(tok_text, orig_text, do_lower_case, verbose_logging)\n","\n","            if final_text in seen_predictions:\n","                continue\n","\n","            seen_predictions[final_text] = True\n","\n","            nbest.append(\n","                _NbestPrediction(text=final_text, start_log_prob=pred.start_log_prob, end_log_prob=pred.end_log_prob)\n","            )\n","\n","        # In very rare edge cases we could have no valid predictions. So we\n","        # just create a nonce prediction in this case to avoid failure.\n","        if not nbest:\n","            nbest.append(_NbestPrediction(text=\"\", start_log_prob=-1e6, end_log_prob=-1e6))\n","\n","        total_scores = []\n","        best_non_null_entry = None\n","        for entry in nbest:\n","            total_scores.append(entry.start_log_prob + entry.end_log_prob)\n","            if not best_non_null_entry:\n","                best_non_null_entry = entry\n","\n","        probs = _compute_softmax(total_scores)\n","\n","        nbest_json = []\n","        for (i, entry) in enumerate(nbest):\n","            output = collections.OrderedDict()\n","            output[\"text\"] = entry.text\n","            output[\"probability\"] = probs[i]\n","            output[\"start_log_prob\"] = entry.start_log_prob\n","            output[\"end_log_prob\"] = entry.end_log_prob\n","            nbest_json.append(output)\n","\n","        assert len(nbest_json) >= 1\n","        assert best_non_null_entry is not None\n","\n","        score_diff = score_null\n","        scores_diff_json[example.qas_id] = score_diff\n","        # note(zhiliny): always predict best_non_null_entry\n","        # and the evaluation script will search for the best threshold\n","        all_predictions[example.qas_id] = best_non_null_entry.text\n","\n","        all_nbest_json[example.qas_id] = nbest_json\n","\n","    with open(output_prediction_file, \"w\") as writer:\n","        writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n","\n","    with open(output_nbest_file, \"w\") as writer:\n","        writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n","\n","    if version_2_with_negative:\n","        with open(output_null_log_odds_file, \"w\") as writer:\n","            writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n","\n","    return all_predictions"],"metadata":{"id":"UtPWlTHm4n7F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def to_list(tensor):\n","    return tensor.detach().cpu().tolist()"],"metadata":{"id":"gC30PzFj4o0L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SquadExample(object):\n","    \"\"\"\n","    A single training/test example for the Squad dataset.\n","    For examples without an answer, the start and end position are -1.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 qas_id,\n","                 question_text,\n","                 doc_tokens,\n","                 orig_answer_text=None,\n","                 start_position=None,\n","                 end_position=None,\n","                 is_impossible=None,\n","                 answers=None):\n","        self.qas_id = qas_id\n","        self.question_text = question_text\n","        self.doc_tokens = doc_tokens\n","        self.orig_answer_text = orig_answer_text\n","        self.start_position = start_position\n","        self.end_position = end_position\n","        self.is_impossible = is_impossible\n","        self.answers = answers\n","\n","    def __str__(self):\n","        return self.__repr__()\n","\n","    def __repr__(self):\n","        s = \"\"\n","        s += \"qas_id: %s\" % (self.qas_id)\n","        s += \", question_text: %s\" % (\n","            self.question_text)\n","        s += \", doc_tokens: [%s]\" % (\" \".join(self.doc_tokens))\n","        if self.start_position:\n","            s += \", start_position: %d\" % (self.start_position)\n","        if self.start_position:\n","            s += \", end_position: %d\" % (self.end_position)\n","        if self.start_position:\n","            s += \", is_impossible: %r\" % (self.is_impossible)\n","        if self.start_position:\n","            s += \", answers: %r\" % (self.answers)\n","        return s"],"metadata":{"id":"ivUhJJiN4tpI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def read_squad_examples(input_file, is_training):\n","    \"\"\"Read a SQuAD json file into a list of SquadExample.\"\"\"\n","    tk = WhitespaceTokenizer()\n","\n","    with open(input_file, \"r\", encoding='utf-8') as reader:\n","        source = json.load(reader)\n","        input_data = source[\"data\"]\n","        version = source[\"version\"]\n","\n","    def is_whitespace(c):\n","        if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n","            return True\n","        return False\n","\n","    examples = []\n","    for entry in input_data:\n","        for paragraph in entry[\"paragraphs\"]:\n","            paragraph_text = paragraph[\"context\"]\n","            doc_tokens = []\n","            char_to_word_offset = []\n","            prev_is_whitespace = True\n","            for c in paragraph_text:\n","                if is_whitespace(c):\n","                    prev_is_whitespace = True\n","                else:\n","                    if prev_is_whitespace:\n","                        doc_tokens.append(c)\n","                    else:\n","                        doc_tokens[-1] += c\n","                    prev_is_whitespace = False\n","                char_to_word_offset.append(len(doc_tokens) - 1)\n","\n","            for qa in paragraph[\"qas\"]:\n","                qas_id = qa[\"id\"]\n","                question_text = qa[\"question\"]\n","                start_position = None\n","                end_position = None\n","                orig_answer_text = None\n","                is_impossible = False\n","                answers = []\n","                if is_training:\n","                    if version == \"v2.0\":\n","                        is_impossible = qa[\"is_impossible\"]\n","                    if (len(qa[\"answers\"]) != 1) and (not is_impossible):\n","                        #print(entry[\"title\"], qas_id)\n","                        raise ValueError(\n","                            \"For training, each question should have exactly 1 answer.\")\n","                    if not is_impossible:\n","                        answer = qa[\"answers\"][0]\n","                        orig_answer_text = answer[\"text\"]\n","                        answer_offset = answer[\"answer_start\"]\n","                        answer_length = len(orig_answer_text)\n","                        start_position = char_to_word_offset[answer_offset]\n","                        end_position = char_to_word_offset[answer_offset + answer_length - 1]\n","                        # Only add answers where the text can be exactly recovered from the\n","                        # document. If this CAN'T happen it's likely due to weird Unicode\n","                        # stuff so we will just skip the example.\n","                        #\n","                        # Note that this means for training mode, every example is NOT\n","                        # guaranteed to be preserved.\n","                        actual_text = \" \".join(doc_tokens[start_position:(end_position + 1)])\n","                        cleaned_answer_text = \" \".join(tk.tokenize(orig_answer_text))\n","                        if actual_text.find(cleaned_answer_text) == -1:\n","                            print(\"Could not find answer: '%s' vs. '%s'\",\n","                                            actual_text, cleaned_answer_text)\n","                            continue\n","                    else:\n","                        start_position = -1\n","                        end_position = -1\n","                        orig_answer_text = \"\"\n","                else:\n","                    answers = qa[\"answers\"]\n","\n","                example = SquadExample(\n","                    qas_id=qas_id,\n","                    question_text=question_text,\n","                    doc_tokens=doc_tokens,\n","                    orig_answer_text=orig_answer_text,\n","                    start_position=start_position,\n","                    end_position=end_position,\n","                    is_impossible=is_impossible,\n","                    answers=answers)\n","                examples.append(example)\n","    return examples\n"],"metadata":{"id":"Bmvf-IuS4wRk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class InputFeatures(object):\n","    \"\"\"\n","    A single set of features of data.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 unique_id,\n","                 example_index,\n","                 doc_span_index,\n","                 tokens,\n","                 token_to_orig_map,\n","                 token_is_max_context,\n","                 input_ids,\n","                 input_mask,\n","                 segment_ids,\n","                 cls_index,\n","                 p_mask,\n","                 paragraph_len,\n","                 start_position=None,\n","                 end_position=None,\n","                 is_impossible=None):\n","\n","        self.unique_id = unique_id\n","        self.example_index = example_index\n","        self.doc_span_index = doc_span_index\n","        self.tokens = tokens\n","        self.token_to_orig_map = token_to_orig_map\n","        self.token_is_max_context = token_is_max_context\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.cls_index = cls_index\n","        self.p_mask = p_mask\n","        self.paragraph_len = paragraph_len\n","        self.start_position = start_position\n","        self.end_position = end_position\n","        self.is_impossible = is_impossible"],"metadata":{"id":"r2bCiZQp4y3y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _improve_answer_span(doc_tokens,\n","                         input_start,\n","                         input_end,\n","                         tokenizer,\n","                         orig_answer_text):\n","    \"\"\"\n","    Returns tokenized answer spans that better match the annotated answer.\n","    \"\"\"\n","\n","    # print(doc_tokens)\n","    # ['beyonce', 'gi', '##selle', 'knowles', '-', 'carter', '(', '/', 'bi', '##ː', '##ˈ', '##j', '##ɒ', '##nse', '##ɪ', '/', 'bee', '-', 'yo', '##n', '-', 'say', ')', '(', 'born', 'september', '4', ',', '1981', ')', 'is', 'an', 'american', 'singer', ',', 'songwriter', ',', 'record', 'producer', 'and', 'actress', '.', 'born', 'and', 'raised', 'in', 'houston', ',', 'texas', ',', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', ',', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'r', '&', 'b', 'girl', '-', 'group', 'destiny', \"'\", 's', 'child', '.', 'managed', 'by', 'her', 'father', ',', 'mathew', 'knowles', ',', 'the', 'group', 'became', 'one', 'of', 'the', 'world', \"'\", 's', 'best', '-', 'selling', 'girl', 'groups', 'of', 'all', 'time', '.', 'their', 'hiatus', 'saw', 'the', 'release', 'of', 'beyonce', \"'\", 's', 'debut', 'album', ',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide', ',', 'earned', 'five', 'grammy', 'awards', 'and', 'featured', 'the', 'billboard', 'hot', '100', 'number', '-', 'one', 'singles', '\"', 'crazy', 'in', 'love', '\"', 'and', '\"', 'baby', 'boy', '\"', '.']\n","\n","    # print(input_start)\n","    # 66\n","\n","    # print(input_end)\n","    # 69\n","\n","    # print(tokenizer)\n","    # <transformers.tokenization_bert.BertTokenizer object at 0x000001C1CE9D4F98>\n","\n","    # print(orig_answer_text)\n","    # in the late 1990s\n","\n","    tok_answer_text = \" \".join(tokenizer.tokenize(orig_answer_text))\n","    # print(tok_answer_text)\n","    # in the late 1990s\n","\n","    for new_start in range(input_start, input_end + 1):\n","        for new_end in range(input_end, new_start - 1, -1):\n","            text_span = \" \".join(doc_tokens[new_start:(new_end + 1)])\n","            # print(text_span)\n","            # in the late 1990s\n","            if text_span == tok_answer_text:\n","                return (new_start, new_end)\n","\n","    return (input_start, input_end)"],"metadata":{"id":"DkRoMZdl41M8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _check_is_max_context(doc_spans, cur_span_index, position):\n","    \"\"\"\n","    Check if this is the \"max context\" doc span for the token.\n","    \"\"\"\n","\n","    best_score = None\n","    best_span_index = None\n","\n","    for (span_index, doc_span) in enumerate(doc_spans):\n","        end = doc_span.start + doc_span.length - 1\n","        if position < doc_span.start:\n","            continue\n","        if position > end:\n","            continue\n","        num_left_context = position - doc_span.start\n","        num_right_context = end - position\n","        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n","        if best_score is None or score > best_score:\n","            best_score = score\n","            best_span_index = span_index\n","\n","    return cur_span_index == best_span_index"],"metadata":{"id":"9T6-j_aJ43Tx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def convert_examples_to_features(examples,\n","                                 tokenizer,\n","                                 max_seq_length,\n","                                 doc_stride,\n","                                 max_query_length,\n","                                 is_training,\n","                                 cls_token_at_end=False,\n","                                 cls_token=\"[CLS]\",\n","                                 sep_token=\"[SEP]\",\n","                                 pad_token=0,\n","                                 sequence_a_segment_id=0,\n","                                 sequence_b_segment_id=1,\n","                                 cls_token_segment_id=0,\n","                                 pad_token_segment_id=0,\n","                                 mask_padding_with_zero=True):\n","    \"\"\"\n","    Loads a data file into a list of `InputBatch`s.\n","    \"\"\"\n","    unique_id = 1000000000\n","\n","    features = []\n","    for (example_index, example) in enumerate(examples):\n","        # print(example_index)\n","        # 0\n","\n","        # print(example)\n","        # qas_id: 56be85543aeaaa14008c9063,\n","        # question_text: When did Beyonce start becoming popular?,\n","        # doc_tokens: [Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny's Child. Managed by her father, Mathew Knowles, the group became one of the world's best-selling girl groups of all time. Their hiatus saw the release of Beyoncé's debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".],\n","        # start_position: 39,\n","        # end_position: 42\n","\n","        query_tokens = tokenizer.tokenize(example.question_text)\n","        # print(query_tokens)\n","        # ['when', 'did', 'beyonce', 'start', 'becoming', 'popular', '?']\n","\n","        if len(query_tokens) > max_query_length:\n","            query_tokens = query_tokens[0:max_query_length]\n","\n","        tok_to_orig_index = []\n","        orig_to_tok_index = []\n","        all_doc_tokens = []\n","\n","        # `token`s are separated by whitespace; `sub_token`s are separated in a `token` by symbol\n","        for (i, token) in enumerate(example.doc_tokens):\n","            # print(i)\n","            # 0\n","\n","            # print(token)\n","            # Beyoncé\n","\n","            orig_to_tok_index.append(len(all_doc_tokens))\n","            sub_tokens = tokenizer.tokenize(token)\n","            # print(sub_tokens)\n","            # ['beyonce']\n","            # ['gi', '##selle']\n","            # ['knowles', '-', 'carter']\n","            # ['(', '/', 'bi', '##ː', '##ˈ', '##j', '##ɒ', '##nse', '##ɪ', '/']\n","            # ...\n","            for sub_token in sub_tokens:\n","                tok_to_orig_index.append(i)\n","                all_doc_tokens.append(sub_token)\n","\n","        # print(tok_to_orig_index)\n","        # [0, 1, 1, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 5, 5, 6, 7, 7, 8, 8, 9, 10, 11, 12, 12, 13, 13, 14, 15, 16, 17, 17, 18, 19, 20, 21, 22, 22, 23, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 47, 47, 48, 48, 48, 49, 49, 49, 50, 50, 51, 52, 53, 54, 54, 55, 56, 56, 57, 58, 59, 60, 61, 62, 63, 63, 63, 64, 64, 64, 65, 66, 67, 68, 69, 69, 70, 71, 72, 73, 74, 75, 76, 76, 76, 77, 78, 78, 79, 80, 81, 82, 82, 82, 82, 83, 84, 85, 86, 87, 88, 89, 90, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 101, 101, 102, 103, 103, 104, 105, 105, 106, 107, 107, 108, 108, 108]\n","\n","        # print(orig_to_tok_index)\n","        # [0, 1, 3, 6, 16, 23, 25, 26, 28, 30, 31, 32, 33, 35, 37, 38, 39, 40, 42, 43, 44, 45, 46, 48, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 77, 80, 83, 85, 86, 87, 88, 90, 91, 93, 94, 95, 96, 97, 98, 99, 102, 105, 106, 107, 108, 109, 111, 112, 113, 114, 115, 116, 117, 120, 121, 123, 124, 125, 126, 130, 131, 132, 133, 134, 135, 136, 137, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 152, 153, 155, 156, 158, 159, 161]\n","\n","        # print(all_doc_tokens)\n","        # ['beyonce', 'gi', '##selle', 'knowles', '-', 'carter', '(', '/', 'bi', '##ː', '##ˈ', '##j', '##ɒ', '##nse', '##ɪ', '/', 'bee', '-', 'yo', '##n', '-', 'say', ')', '(', 'born', 'september', '4', ',', '1981', ')', 'is', 'an', 'american', 'singer', ',', 'songwriter', ',', 'record', 'producer', 'and', 'actress', '.', 'born', 'and', 'raised', 'in', 'houston', ',', 'texas', ',', 'she', 'performed', 'in', 'various', 'singing', 'and', 'dancing', 'competitions', 'as', 'a', 'child', ',', 'and', 'rose', 'to', 'fame', 'in', 'the', 'late', '1990s', 'as', 'lead', 'singer', 'of', 'r', '&', 'b', 'girl', '-', 'group', 'destiny', \"'\", 's', 'child', '.', 'managed', 'by', 'her', 'father', ',', 'mathew', 'knowles', ',', 'the', 'group', 'became', 'one', 'of', 'the', 'world', \"'\", 's', 'best', '-', 'selling', 'girl', 'groups', 'of', 'all', 'time', '.', 'their', 'hiatus', 'saw', 'the', 'release', 'of', 'beyonce', \"'\", 's', 'debut', 'album', ',', 'dangerously', 'in', 'love', '(', '2003', ')', ',', 'which', 'established', 'her', 'as', 'a', 'solo', 'artist', 'worldwide', ',', 'earned', 'five', 'grammy', 'awards', 'and', 'featured', 'the', 'billboard', 'hot', '100', 'number', '-', 'one', 'singles', '\"', 'crazy', 'in', 'love', '\"', 'and', '\"', 'baby', 'boy', '\"', '.']\n","\n","        tok_start_position = None\n","        tok_end_position = None\n","\n","        if is_training and example.is_impossible:\n","            tok_start_position = -1\n","            tok_end_position = -1\n","        if is_training and not example.is_impossible:\n","            tok_start_position = orig_to_tok_index[example.start_position]\n","            if example.end_position < len(example.doc_tokens) - 1:\n","                tok_end_position = orig_to_tok_index[example.end_position + 1] - 1\n","            else:\n","                tok_end_position = len(all_doc_tokens) - 1\n","            # print(tok_start_position)\n","            # 66\n","\n","            # print(tok_end_position)\n","            # 69\n","            (tok_start_position, tok_end_position) = _improve_answer_span(all_doc_tokens,\n","                                                                          tok_start_position,\n","                                                                          tok_end_position,\n","                                                                          tokenizer,\n","                                                                          example.orig_answer_text)\n","            # print(tok_start_position)\n","            # 66\n","\n","            # print(tok_end_position)\n","            # 69\n","\n","        # The -3 accounts for [CLS], [SEP] and [SEP]\n","        max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n","\n","        # We can have documents that are longer than the maximum sequence length. To deal with this we do a\n","        # sliding window approach, where we take chunks of the up to our max length with a stride of `doc_stride`.\n","        _DocSpan = collections.namedtuple(\"DocSpan\", [\"start\", \"length\"])\n","        doc_spans = []\n","        start_offset = 0\n","\n","        while start_offset < len(all_doc_tokens):\n","            # print(len(all_doc_tokens))\n","            # 426\n","            length = len(all_doc_tokens) - start_offset\n","            if length > max_tokens_for_doc:\n","                length = max_tokens_for_doc\n","            doc_spans.append(_DocSpan(start=start_offset, length=length))\n","            # Take an example with stride\n","            #\n","            # print(doc_spans)\n","            # [DocSpan(start=0, length=373)]\n","            #\n","            # In this case, `start` will move a `doc_strike`, 128, so the new `start` is 128\n","            # And the new `length` is 426 - 128 = 298\n","            #\n","            # [DocSpan(start=0, length=373), DocSpan(start=128, length=298)]\n","            if start_offset + length == len(all_doc_tokens):\n","                break\n","            start_offset += min(length, doc_stride)\n","\n","        for (doc_span_index, doc_span) in enumerate(doc_spans):\n","\n","            tokens = []\n","            token_to_orig_map = {}\n","            token_is_max_context = {}\n","            segment_ids = []\n","            # `p_mask`: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n","            # Original TF implem also keeps the classification token (set to 0) (not sure why...)\n","            p_mask = []\n","\n","            # `[CLS]` token at the beginning\n","            if not cls_token_at_end:\n","                tokens.append(cls_token)\n","                segment_ids.append(cls_token_segment_id)\n","                p_mask.append(0)\n","                cls_index = 0\n","\n","            # Query\n","            for token in query_tokens:\n","                tokens.append(token)\n","                segment_ids.append(sequence_a_segment_id)\n","                p_mask.append(1)\n","\n","            # [SEP] token\n","            tokens.append(sep_token)\n","            segment_ids.append(sequence_a_segment_id)\n","            p_mask.append(1)\n","\n","            # Paragraph built based on `doc_span`\n","            for i in range(doc_span.length):\n","                split_token_index = doc_span.start + i\n","                token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n","                is_max_context = _check_is_max_context(doc_spans, doc_span_index, split_token_index)\n","                token_is_max_context[len(tokens)] = is_max_context\n","                tokens.append(all_doc_tokens[split_token_index])\n","                segment_ids.append(sequence_b_segment_id)\n","                p_mask.append(0)\n","            paragraph_len = doc_span.length\n","\n","            # [SEP] token\n","            tokens.append(sep_token)\n","            segment_ids.append(sequence_b_segment_id)\n","            p_mask.append(1)\n","\n","            # [CLS] token at the end\n","            if cls_token_at_end:\n","                tokens.append(cls_token)\n","                segment_ids.append(cls_token_segment_id)\n","                p_mask.append(0)\n","                # Index of classification token\n","                cls_index = len(tokens) - 1\n","\n","            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","            # The mask has 1 for real tokens and 0 for padding tokens. Only real tokens are attended to.\n","            input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","            # Zero-pad up to the sequence length.\n","            while len(input_ids) < max_seq_length:\n","                input_ids.append(pad_token)\n","                input_mask.append(0 if mask_padding_with_zero else 1)\n","                segment_ids.append(pad_token_segment_id)\n","                p_mask.append(1)\n","\n","            # print(input_ids)\n","            # [101, 2043, 2106, 20773, 2707, 3352, 2759, 1029, 102, 20773, 21025, 19358, 22815, 1011, 5708, 1006, 1013, 12170, 23432, 29715, 3501, 29678, 12325, 29685, 1013, 10506, 1011, 10930, 2078, 1011, 2360, 1007, 1006, 2141, 2244, 1018, 1010, 3261, 1007, 2003, 2019, 2137, 3220, 1010, 6009, 1010, 2501, 3135, 1998, 3883, 1012, 2141, 1998, 2992, 1999, 5395, 1010, 3146, 1010, 2016, 2864, 1999, 2536, 4823, 1998, 5613, 6479, 2004, 1037, 2775, 1010, 1998, 3123, 2000, 4476, 1999, 1996, 2397, 4134, 2004, 2599, 3220, 1997, 1054, 1004, 1038, 2611, 1011, 2177, 10461, 1005, 1055, 2775, 1012, 3266, 2011, 2014, 2269, 1010, 25436, 22815, 1010, 1996, 2177, 2150, 2028, 1997, 1996, 2088, 1005, 1055, 2190, 1011, 4855, 2611, 2967, 1997, 2035, 2051, 1012, 2037, 14221, 2387, 1996, 2713, 1997, 20773, 1005, 1055, 2834, 2201, 1010, 20754, 1999, 2293, 1006, 2494, 1007, 1010, 2029, 2511, 2014, 2004, 1037, 3948, 3063, 4969, 1010, 3687, 2274, 8922, 2982, 1998, 2956, 1996, 4908, 2980, 2531, 2193, 1011, 2028, 3895, 1000, 4689, 1999, 2293, 1000, 1998, 1000, 3336, 2879, 1000, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","            # print(input_mask)\n","            # [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","\n","            # print(segment_ids)\n","            # [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n","            # Only `sequence_b_segment_id` is set to 1\n","\n","            assert len(input_ids) == max_seq_length\n","            assert len(input_mask) == max_seq_length\n","            assert len(segment_ids) == max_seq_length\n","\n","            span_is_impossible = example.is_impossible\n","            start_position = None\n","            end_position = None\n","\n","            # Get `start_position` and `end_position`\n","            if is_training and not span_is_impossible:\n","                # For training, if our document chunk does not contain an annotation we throw it out,\n","                # since there is nothing to predict.\n","                doc_start = doc_span.start\n","                doc_end = doc_span.start + doc_span.length - 1\n","                out_of_span = False\n","                if not (tok_start_position >= doc_start and\n","                        tok_end_position <= doc_end):\n","                    out_of_span = True\n","                if out_of_span:\n","                    start_position = 0\n","                    end_position = 0\n","                    span_is_impossible = True\n","                else:\n","                    doc_offset = len(query_tokens) + 2\n","                    start_position = tok_start_position - doc_start + doc_offset\n","                    end_position = tok_end_position - doc_start + doc_offset\n","\n","            if is_training and span_is_impossible:\n","                start_position = cls_index\n","                end_position = cls_index\n","\n","            # Display some examples\n","            if example_index < 20:\n","                print(\"*** Example ***\")\n","                # *** Example ***\n","                print(\"unique_id: %s\" % (unique_id))\n","                # unique_id: 1000000000\n","                print(\"example_index: %s\" % (example_index))\n","                # example_index: 0\n","                print(\"doc_span_index: %s\" % (doc_span_index))\n","                # doc_span_index: 0\n","                print(\"tokens: %s\" % \" \".join(tokens))\n","                # tokens: [CLS] when did beyonce start becoming popular ? [SEP] beyonce gi ##selle knowles - carter ( / bi ##ː ##ˈ ##j ##ɒ ##nse ##ɪ / bee - yo ##n - say ) ( born september 4 , 1981 ) is an american singer , songwriter , record producer and actress . born and raised in houston , texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of r & b girl - group destiny ' s child . managed by her father , mathew knowles , the group became one of the world ' s best - selling girl groups of all time . their hiatus saw the release of beyonce ' s debut album , dangerously in love ( 2003 ) , which established her as a solo artist worldwide , earned five grammy awards and featured the billboard hot 100 number - one singles \" crazy in love \" and \" baby boy \" . [SEP]\n","                print(\"token_to_orig_map: %s\" % \" \".join([\n","                    \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n","                # token_to_orig_map: 9:0 10:1 11:1 12:2 13:2 14:2 15:3 16:3 17:3 18:3 19:3 20:3 21:3 22:3 23:3 24:3 25:4 26:4 27:4 28:4 29:4 30:4 31:4 32:5 33:5 34:6 35:7 36:7 37:8 38:8 39:9 40:10 41:11 42:12 43:12 44:13 45:13 46:14 47:15 48:16 49:17 50:17 51:18 52:19 53:20 54:21 55:22 56:22 57:23 58:23 59:24 60:25 61:26 62:27 63:28 64:29 65:30 66:31 67:32 68:33 69:34 70:34 71:35 72:36 73:37 74:38 75:39 76:40 77:41 78:42 79:43 80:44 81:45 82:46 83:47 84:47 85:47 86:48 87:48 88:48 89:49 90:49 91:49 92:50 93:50 94:51 95:52 96:53 97:54 98:54 99:55 100:56 101:56 102:57 103:58 104:59 105:60 106:61 107:62 108:63 109:63 110:63 111:64 112:64 113:64 114:65 115:66 116:67 117:68 118:69 119:69 120:70 121:71 122:72 123:73 124:74 125:75 126:76 127:76 128:76 129:77 130:78 131:78 132:79 133:80 134:81 135:82 136:82 137:82 138:82 139:83 140:84 141:85 142:86 143:87 144:88 145:89 146:90 147:90 148:91 149:92 150:93 151:94 152:95 153:96 154:97 155:98 156:99 157:100 158:101 159:101 160:101 161:102 162:103 163:103 164:104 165:105 166:105 167:106 168:107 169:107 170:108 171:108 172:108\n","                print(\"token_is_max_context: %s\" % \" \".join([\n","                    \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n","                ]))\n","                # token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True\n","                print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n","                # input_ids: 101 2043 2106 20773 2707 3352 2759 1029 102 20773 21025 19358 22815 1011 5708 1006 1013 12170 23432 29715 3501 29678 12325 29685 1013 10506 1011 10930 2078 1011 2360 1007 1006 2141 2244 1018 1010 3261 1007 2003 2019 2137 3220 1010 6009 1010 2501 3135 1998 3883 1012 2141 1998 2992 1999 5395 1010 3146 1010 2016 2864 1999 2536 4823 1998 5613 6479 2004 1037 2775 1010 1998 3123 2000 4476 1999 1996 2397 4134 2004 2599 3220 1997 1054 1004 1038 2611 1011 2177 10461 1005 1055 2775 1012 3266 2011 2014 2269 1010 25436 22815 1010 1996 2177 2150 2028 1997 1996 2088 1005 1055 2190 1011 4855 2611 2967 1997 2035 2051 1012 2037 14221 2387 1996 2713 1997 20773 1005 1055 2834 2201 1010 20754 1999 2293 1006 2494 1007 1010 2029 2511 2014 2004 1037 3948 3063 4969 1010 3687 2274 8922 2982 1998 2956 1996 4908 2980 2531 2193 1011 2028 3895 1000 4689 1999 2293 1000 1998 1000 3336 2879 1000 1012 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","                print(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n","                # input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","                print(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n","                # segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","                if is_training and span_is_impossible:\n","                    print(\"impossible example\")\n","                if is_training and not span_is_impossible:\n","                    answer_text = \" \".join(tokens[start_position:(end_position + 1)])\n","                    print(\"start_position: %d\" % (start_position))\n","                    # start_position: 75\n","                    print(\"end_position: %d\" % (end_position))\n","                    # end_position: 78\n","                    print(\"answer: %s\" % (answer_text))\n","                    # answer: in the late 1990s\n","\n","            features.append(\n","                InputFeatures(\n","                    unique_id=unique_id,\n","                    example_index=example_index,\n","                    doc_span_index=doc_span_index,\n","                    tokens=tokens,\n","                    token_to_orig_map=token_to_orig_map,\n","                    token_is_max_context=token_is_max_context,\n","                    input_ids=input_ids,\n","                    input_mask=input_mask,\n","                    segment_ids=segment_ids,\n","                    cls_index=cls_index,\n","                    p_mask=p_mask,\n","                    paragraph_len=paragraph_len,\n","                    start_position=start_position,\n","                    end_position=end_position,\n","                    is_impossible=span_is_impossible)\n","            )\n","\n","            unique_id += 1\n","\n","    return features"],"metadata":{"id":"3yt-M_3544Rj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def load_and_cache_examples(args,\n","                            tokenizer,\n","                            evaluate=False,\n","                            output_examples=False):\n","\n","    # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","    if args[\"local_rank\"] not in [-1, 0] and not evaluate:\n","        torch.distributed.barrier()\n","\n","    # Load data features from cache or dataset file\n","    input_file = args[\"predict_file\"] if evaluate else args[\"train_file\"]\n","    cached_features_file = os.path.join(\n","        os.path.dirname(input_file),\n","        'cached_{}_{}_{}'.format(\n","            'dev' if evaluate else 'train',\n","            list(filter(None, args[\"model_name_or_path\"].split('/'))).pop(),\n","            str(args[\"max_seq_length\"])\n","        )\n","    )\n","\n","    if os.path.exists(cached_features_file) and not args[\"overwrite_cache\"] and not output_examples:\n","        print(\"Loading features from cached file %s\", cached_features_file)\n","        features = torch.load(cached_features_file)\n","    else:\n","        print(\"Creating features from dataset file at %s\", input_file)\n","\n","        # Call `read_squad_examples()`\n","        examples = read_squad_examples(input_file=input_file,\n","                                       is_training=not evaluate)\n","\n","        # Call `convert_examples_to_features()`\n","        features = convert_examples_to_features(examples,\n","                                                tokenizer,\n","                                                max_seq_length=args[\"max_seq_length\"],\n","                                                doc_stride=args[\"doc_stride\"],\n","                                                max_query_length=args[\"max_query_length\"],\n","                                                is_training=not evaluate,\n","                                                cls_token_at_end=False,\n","                                                cls_token=\"[CLS]\",\n","                                                sep_token=\"[SEP]\",\n","                                                pad_token=0,\n","                                                sequence_a_segment_id=0, # 'pad_token': '[PAD]'\n","                                                sequence_b_segment_id=1,\n","                                                cls_token_segment_id=0,\n","                                                pad_token_segment_id=0,\n","                                                mask_padding_with_zero=True)\n","\n","        if args[\"local_rank\"] in [-1, 0]:\n","            print(\"Saving features into cached file %s\", cached_features_file)\n","            torch.save(features, cached_features_file)\n","\n","    # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","    if args[\"local_rank\"] == 0 and not evaluate:\n","        torch.distributed.barrier()\n","\n","    # Convert to Tensors and build dataset\n","    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n","    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n","    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n","    all_cls_index = torch.tensor([f.cls_index for f in features], dtype=torch.long)\n","    all_p_mask = torch.tensor([f.p_mask for f in features], dtype=torch.float)\n","\n","    if evaluate:\n","        all_example_index = torch.arange(all_input_ids.size(0), dtype=torch.long)\n","        dataset = TensorDataset(all_input_ids,\n","                                all_input_mask,\n","                                all_segment_ids,\n","                                all_example_index,\n","                                all_cls_index,\n","                                all_p_mask)\n","    else:\n","        all_start_positions = torch.tensor([f.start_position for f in features], dtype=torch.long)\n","        all_end_positions = torch.tensor([f.end_position for f in features], dtype=torch.long)\n","        dataset = TensorDataset(all_input_ids,\n","                                all_input_mask,\n","                                all_segment_ids,\n","                                all_start_positions,\n","                                all_end_positions,\n","                                all_cls_index,\n","                                all_p_mask)\n","\n","    if output_examples:\n","        return dataset, examples, features\n","\n","    return dataset"],"metadata":{"id":"74n5DX8t48bK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def set_seed(args):\n","    random.seed(args[\"seed\"])\n","    np.random.seed(args[\"seed\"])\n","    torch.manual_seed(args[\"seed\"])\n","    if args[\"n_gpu\"] > 0:\n","        torch.cuda.manual_seed_all(args[\"seed\"])"],"metadata":{"id":"3FQTpCwE5Azg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(args,\n","          train_dataset,\n","          model,\n","          tokenizer):\n","    \"\"\"\n","    Train the model.\n","    \"\"\"\n","\n","    args[\"train_batch_size\"] = args[\"per_gpu_train_batch_size\"] * max(1, args[\"n_gpu\"])\n","    train_sampler = RandomSampler(train_dataset) if args[\"local_rank\"] == -1 else DistributedSampler(train_dataset)\n","    train_dataloader = DataLoader(train_dataset,\n","                                  sampler=train_sampler,\n","                                  batch_size=args[\"train_batch_size\"])\n","\n","    if args[\"max_steps\"] > 0:\n","        t_total = args[\"max_steps\"]\n","        args[\"num_train_epochs\"] = args[\"max_steps\"] // (len(train_dataloader) // args[\"gradient_accumulation_steps\"]) + 1\n","    else:\n","        t_total = len(train_dataloader) // args[\"gradient_accumulation_steps\"] * args[\"num_train_epochs\"]\n","\n","    # Prepare optimizer and scheduler (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': args[\"weight_decay\"]},\n","        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","    ]\n","    optimizer = AdamW(optimizer_grouped_parameters,\n","                      lr=args[\"learning_rate\"],\n","                      eps=args[\"adam_epsilon\"])\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                     num_warmup_steps=args[\"warmup_steps\"],\n","                                     num_training_steps=t_total)\n","\n","    # Multiple GPU training\n","    if args[\"n_gpu\"] > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training\n","    if args[\"local_rank\"] != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(model,\n","                                                          device_ids=[args[\"local_rank\"]],\n","                                                          output_device=args[\"local_rank\"],\n","                                                          find_unused_parameters=True)\n","\n","    # Training\n","    print(\"***** Running training *****\")\n","    print(\"  Num examples = %d\", len(train_dataset))\n","    print(\"  Num Epochs = %d\", args[\"num_train_epochs\"])\n","    print(\"  Instantaneous batch size per GPU = %d\", args[\"per_gpu_train_batch_size\"])\n","    print(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","                   args[\"train_batch_size\"] * args[\"gradient_accumulation_steps\"] * (torch.distributed.get_world_size() if args[\"local_rank\"] != -1 else 1))\n","    print(\"  Gradient Accumulation steps = %d\", args[\"gradient_accumulation_steps\"])\n","    print(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    # Added here for reproductibility (even between python 2 and 3)\n","    set_seed(args)\n","    for _ in range(int(args[\"num_train_epochs\"])):\n","        for step, batch in enumerate(train_dataloader):\n","            model.train()\n","            batch = tuple(t.to(args[\"device\"]) for t in batch)\n","            inputs = {'input_ids':       batch[0],\n","                      'attention_mask':  batch[1],\n","                      'start_positions': batch[3],\n","                      'end_positions':   batch[4]}\n","            if args[\"model_type\"] != 'distilbert':\n","                inputs['token_type_ids'] = None if args[\"model_type\"] == 'xlm' else batch[2]\n","            if args[\"model_type\"] in ['xlnet', 'xlm']:\n","                inputs.update({'cls_index': batch[5],\n","                               'p_mask': batch[6]})\n","            outputs = model(**inputs)\n","            # Model outputs are always tuple in transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args[\"n_gpu\"] > 1:\n","                # `mean()` to average on multi-gpu parallel (not distributed) training\n","                loss = loss.mean()\n","            if args[\"gradient_accumulation_steps\"] > 1:\n","                loss = loss / args[\"gradient_accumulation_steps\"]\n","\n","            loss.backward()\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), args[\"max_grad_norm\"])\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args[\"gradient_accumulation_steps\"] == 0:\n","                optimizer.step()\n","                # Update learning rate schedule\n","                scheduler.step()\n","                model.zero_grad()\n","                global_step += 1\n","\n","                #if args[\"local_rank\"] in [-1, 0] and args[\"logging_steps\"] > 0 and global_step % args[\"logging_steps\"] == 0:\n","                    # Log metrics\n","                    # Only evaluate when single GPU otherwise metrics may not average well\n","                    #if local_rank == -1 and evaluate_during_training:\n","                    #    results = evaluate(args, model, tokenizer)\n","                    #    for key, value in results.items():\n","                    #        tb_writer.add_scalar('eval_{}'.format(key), value, global_step)\n","                    #tb_writer.add_scalar('lr', scheduler.get_lr()[0], global_step)\n","                    #tb_writer.add_scalar('loss', (tr_loss - logging_loss)/args[\"logging_steps\"], global_step)\n","                    #logging_loss = tr_loss\n","\n","                if args[\"local_rank\"] in [-1, 0] and args[\"save_steps\"] > 0 and global_step % args[\"save_steps\"] == 0:\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(args[\"output_dir\"], 'checkpoint-{}'.format(global_step))\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    # Take care of distributed/parallel training\n","                    model_to_save = model.module if hasattr(model, 'module') else model\n","                    model_to_save.save_pretrained(output_dir)\n","                    torch.save(args, os.path.join(output_dir, 'training_args.bin'))\n","                    print(\"Saving model checkpoint to %s\", output_dir)\n","\n","            if args[\"max_steps\"] > 0 and global_step > args[\"max_steps\"]:\n","                epoch_iterator.close()\n","                break\n","\n","        if args[\"max_steps\"] > 0 and global_step > args[\"max_steps\"]:\n","            train_iterator.close()\n","            break\n","\n","    #if args[\"local_rank\"] in [-1, 0]:\n","    #    tb_writer.close()\n","\n","    return global_step, tr_loss / global_step"],"metadata":{"id":"dgmQkLxc5C49"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DATASET_URL = '/content/train_ViQuAD.json'\n","MODEL_NAME = 'bert-base-multilingual-cased'\n","MAX_LEN = 384\n","\n","# Set language name to save model\n","LANGUAGE = 'Vietnam'\n","\n","# Depends on whether we are using TPUs or not, increase BATCH_SIZE\n","\n","ARTIFACTS_PATH = 'content/artifacts/'\n","\n","if not os.path.exists(ARTIFACTS_PATH):\n","    os.makedirs(ARTIFACTS_PATH)"],"metadata":{"id":"GWjS6skvhjwd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MODEL_CLASSES = {\n","    'bert': (BertConfig, BertForQuestionAnswering, BertTokenizerFast),\n","}\n","\n","args = {\n","    \"local_rank\": -1,\n","    \"model_type\": \"bert\",\n","    \"config_name\": \"\",\n","    \"model_name_or_path\": \"bert-base-multilingual-cased\",\n","    \"tokenizer_name\": \"\",\n","    \"max_seq_length\": 384,\n","    \"overwrite_cache\": False,\n","    \"do_lower_case\": True,\n","    \"do_train\": True,\n","    \"output_dir\": \"models/bert-finetuned-vinews1\",\n","    \"version_2_with_negative\": True,\n","    \"doc_stride\": 128,\n","    \"max_query_length\": 64,\n","    \"train_file\": \"./ViQuAD1.0/train_ViQuAD.json\",\n","    \"predict_file\": \"./ViQuAD1.0/dev_ViQuAD.json\",\n","    \"per_gpu_train_batch_size\": 4,\n","    \"max_steps\": -1,\n","    \"num_train_epochs\": 3,\n","    \"learning_rate\": 3e-5,\n","    \"adam_epsilon\": 1e-8,\n","    \"warmup_steps\": 100,\n","    \"no_cuda\": False,\n","    \"gradient_accumulation_steps\": 1,\n","    \"max_grad_norm\": 1.0,\n","    \"weight_decay\": 0.0,\n","    \"save_steps\": 1000,\n","    \"seed\": 42,\n","    \"do_eval\": True,\n","    \"eval_all_checkpoints\": True,\n","    \"eval_batch_size\": 8,\n","    \"per_gpu_eval_batch_size\": 4,\n","    \"n_best_size\": 20,\n","    \"max_answer_length\": 300,\n","    \"null_score_diff_threshold\": 0.0,\n","    \"verbose_logging\": True,\n","    \"version_2_with_negative\": True\n","\n","}\n","\n","if args[\"local_rank\"] == -1 or args[\"no_cuda\"]:\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"tpu\")\n","    args[\"n_gpu\"] = torch.cuda.device_count()\n","else:\n","    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","    torch.cuda.set_device(args[\"local_rank\"])\n","    device = torch.device(\"cuda\", args[\"local_rank\"])\n","    torch.distributed.init_process_group(backend='nccl')\n","    args[\"n_gpu\"] = 1\n","\n","args[\"device\"] = device\n","\n","config_class, model_class, tokenizer_class = MODEL_CLASSES[args[\"model_type\"]]\n","config = config_class.from_pretrained(args[\"config_name\"] if args[\"config_name\"] else args[\"model_name_or_path\"])\n","tokenizer = tokenizer_class.from_pretrained(args[\"tokenizer_name\"] if args[\"tokenizer_name\"] else args[\"model_name_or_path\"], do_lower_case=args[\"do_lower_case\"], strip_accents=False, add_special_tokens=False)\n","model = model_class.from_pretrained(args[\"model_name_or_path\"], from_tf=bool('.ckpt' in args[\"model_name_or_path\"]), config=config)\n","\n","# Make sure only the first process in distributed training will download model & vocab\n","if args[\"local_rank\"] == 0:\n","    torch.distributed.barrier()\n","\n","model.to(args[\"device\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["41e119fc46a44f1c9dee1d4435658950","202b360550a54ec0b40cc57d3b92718b","748021f1c28f469fbb82f94c88cd30cb","b8f8d1fffa684f50aa6e554d3ceb9bfb","2836d2d9585c46799b66e7b413e094e7","d40c3d06f75440f1beb79c5d784435c5","e31a71ba8d3341c08700faf671328722","eaa155a97176432ca2b52e866338a856","d3aa52ca635f4227b873fa159af5a3c5","124e84dbd3b045db84a459c68d7d7000","aab3a951cf89466f97f22d8201ed6e37","474167ec9f804f31b829d3f6f275c75b","94950f1a888f4b5f949ff924125d5fcd","c58a4be62e4440028c3aa91017648b9b","9d1b81a61be24c28930fcf6199507d7d","ad5362e29ab745d7939b7efd7479dab0","a94fd7bed1014dd8ab8930e84db06e24","b92037f7295d42bebb7cddf610c9aba6","b923b5f56a4d49dfbfca6b84b36c799a","a210fec3b3484589b7cd1793f06125d3","ec3b4e8d634b4593999b1d4ccd4626af","da674ec5e7754911aa9ddd9b74f8fda2","f5fa515d5b794d319dba563b2d86995d","33c0b0efc56940dcb5b6a4339199c489","023ca90d48994cfa8b5d5b0ce66776c7","58ab23e652d742139e2b304905c86faf","01c5c7019fad482ea1cf1cbb878f7626","144a86dbf7f94bfd969edbbde0cac55e","552e8bf517e349b99474e9a812306956","a46f89f0e90f4f87afcaf8ca43b66603","2ac33d7327b74281b68a6dcad31fb7d2","f21e3ab19af9455e9052185db814452c","3472d917ebde422f906dbcadaf2ef7cc","0209a91c0a874808aa234633d9399e72","b56238605a284bc18a91257f112914e2","01f1aae1b98b425cb0661ad1092ff350","7df45ffc3c0b498baa3f65abb932a109","15c5afdc87154de2bb485c8706e3931a","10cdf9bf4f1048c282fdc0ea5e9426e6","91b6cef80b7943c8a8be259757779e2b","66a78613a4f540849d376e9335016885","d723f97cb4d8467bb23574f9b5879939","f2f07d9271824fa6bcec9c481865e41c","7e981c0f304c42ef91c4d1a9e77f53d7","43656fa1eb4346c582d2f8845d8a81c9","83ae1ac556f643d7972fed2e1dcf8c2e","da94720645a3472499f9a5ff2cd29020","2e62f55051684830bc4a27b754456213","e75729715eda4493bbe62494af8a29fb","da4428afdf824430acd7727c7be3a58e","2c2d2120bafa4041ad110587d592bdab","1ffe5f808fdd44bb94af72de77615e2c","590b8efcfedb40c0a33ea508983ba60e","a8cdea7318264995a7a582b3e14ddfc5","054ec626678a4558a1ccee0289279306"]},"id":"AcxoJepq5FvL","executionInfo":{"status":"ok","timestamp":1707419094043,"user_tz":-420,"elapsed":12100,"user":{"displayName":"Toàn Nguyễn Văn","userId":"12622235312397842785"}},"outputId":"837e09b0-c249-46a3-a10c-0e20436b4ae3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41e119fc46a44f1c9dee1d4435658950"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"474167ec9f804f31b829d3f6f275c75b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5fa515d5b794d319dba563b2d86995d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0209a91c0a874808aa234633d9399e72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43656fa1eb4346c582d2f8845d8a81c9"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForQuestionAnswering(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(119547, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0-11): 12 x BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",")"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["def evaluate(args, model, tokenizer, prefix=\"\"):\n","    dataset, examples, features = load_and_cache_examples(args, tokenizer, evaluate=True, output_examples=True)\n","\n","    if not os.path.exists(args[\"output_dir\"]) and args[\"local_rank\"] in [-1, 0]:\n","        os.makedirs(args[\"output_dir\"])\n","\n","    args[\"eval_batch_size\"] = args[\"per_gpu_eval_batch_size\"] * max(1, args[\"n_gpu\"])\n","\n","    # Note that DistributedSampler samples randomly\n","    eval_sampler = SequentialSampler(dataset)\n","    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=args[\"eval_batch_size\"])\n","\n","    # multi-gpu evaluate\n","    if args[\"n_gpu\"] > 1 and not isinstance(model, torch.nn.DataParallel):\n","        model = torch.nn.DataParallel(model)\n","\n","    # Eval!\n","    print(\"***** Running evaluation {} *****\".format(prefix))\n","    print(\"  Num examples = %d\", len(dataset))\n","    print(\"  Batch size = %d\", args[\"eval_batch_size\"])\n","\n","    all_results = []\n","    start_time = timeit.default_timer()\n","\n","    for batch in eval_dataloader:\n","        model.eval()\n","        batch = tuple(t.to(args[\"device\"]) for t in batch)\n","\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0],\n","                \"attention_mask\": batch[1],\n","                \"token_type_ids\": batch[2],\n","            }\n","\n","            if args[\"model_type\"] in [\"xlm\", \"roberta\", \"distilbert\", \"camembert\"]:\n","                del inputs[\"token_type_ids\"]\n","\n","            example_indices = batch[3]\n","\n","            # XLNet and XLM use more arguments for their predictions\n","            if args[\"model_type\"] in [\"xlnet\", \"xlm\"]:\n","                inputs.update({\"cls_index\": batch[4], \"p_mask\": batch[5]})\n","                # for lang_id-sensitive xlm models\n","                if hasattr(model, \"config\") and hasattr(model.config, \"lang2id\"):\n","                    inputs.update(\n","                        {\"langs\": (torch.ones(batch[0].shape, dtype=torch.int64) * args[\"lang_id\"]).to(args[\"device\"])}\n","                    )\n","\n","            outputs = model(**inputs)\n","\n","        for i, example_index in enumerate(example_indices):\n","            eval_feature = features[example_index.item()]\n","            unique_id = int(eval_feature.unique_id)\n","\n","            output = [to_list(output[i]) for output in outputs]\n","\n","            # Some models (XLNet, XLM) use 5 arguments for their predictions, while the other \"simpler\"\n","            # models only use two.\n","            if len(output) >= 5:\n","                start_logits = output[0]\n","                start_top_index = output[1]\n","                end_logits = output[2]\n","                end_top_index = output[3]\n","                cls_logits = output[4]\n","\n","                result = SquadResult(\n","                    unique_id,\n","                    start_logits,\n","                    end_logits,\n","                    start_top_index=start_top_index,\n","                    end_top_index=end_top_index,\n","                    cls_logits=cls_logits,\n","                )\n","\n","            else:\n","                start_logits, end_logits = output\n","                result = SquadResult(unique_id, start_logits, end_logits)\n","\n","            all_results.append(result)\n","\n","    evalTime = timeit.default_timer() - start_time\n","    print(\"  Evaluation done in total %f secs (%f sec per example)\", evalTime, evalTime / len(dataset))\n","\n","    # Compute predictions\n","    output_prediction_file = os.path.join(args[\"output_dir\"], \"predictions_{}.json\".format(prefix))\n","    output_nbest_file = os.path.join(args[\"output_dir\"], \"nbest_predictions_{}.json\".format(prefix))\n","\n","    if args[\"version_2_with_negative\"]:\n","        output_null_log_odds_file = os.path.join(args[\"output_dir\"], \"null_odds_{}.json\".format(prefix))\n","    else:\n","        output_null_log_odds_file = None\n","\n","    # XLNet and XLM use a more complex post-processing procedure\n","    if args[\"model_type\"] in [\"xlnet\", \"xlm\"]:\n","        start_n_top = model.config.start_n_top if hasattr(model, \"config\") else model.module.config.start_n_top\n","        end_n_top = model.config.end_n_top if hasattr(model, \"config\") else model.module.config.end_n_top\n","\n","        predictions = compute_predictions_log_probs(\n","            examples,\n","            features,\n","            all_results,\n","            args[\"n_best_size\"],\n","            args[\"max_answer_length\"],\n","            output_prediction_file,\n","            output_nbest_file,\n","            output_null_log_odds_file,\n","            start_n_top,\n","            end_n_top,\n","            args[\"version_2_with_negative\"],\n","            tokenizer,\n","            args[\"verbose_logging\"],\n","        )\n","    else:\n","        predictions = compute_predictions_logits(\n","            examples,\n","            features,\n","            all_results,\n","            args[\"n_best_size\"],\n","            args[\"max_answer_length\"],\n","            args[\"do_lower_case\"],\n","            output_prediction_file,\n","            output_nbest_file,\n","            output_null_log_odds_file,\n","            args[\"verbose_logging\"],\n","            args[\"version_2_with_negative\"],\n","            args[\"null_score_diff_threshold\"],\n","            tokenizer,\n","        )\n","\n","    # Compute the F1 and exact scores.\n","    results = squad_evaluate(examples, predictions)\n","    return results"],"metadata":{"id":"RB5ictuKI5ju"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Training/evaluation parameters %s\", args)\n","\n","# Training\n","if args[\"do_train\"]:\n","    train_dataset = load_and_cache_examples(args,\n","                                            tokenizer,\n","                                            evaluate=False,\n","                                            output_examples=False)\n","    global_step, tr_loss = train(args,\n","                                 train_dataset,\n","                                 model,\n","                                 tokenizer)\n","    print(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","# Save the trained model and the tokenizer\n","if args[\"do_train\"] and (args[\"local_rank\"] == -1 or torch.distributed.get_rank() == 0):\n","    # Create output directory if needed\n","    if not os.path.exists(args[\"output_dir\"]) and args[\"local_rank\"] in [-1, 0]:\n","        os.makedirs(args[\"output_dir\"])\n","\n","    print(\"Saving model checkpoint to %s\", args[\"output_dir\"])\n","    # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","    # They can then be reloaded using `from_pretrained()`\n","    # Take care of distributed/parallel training\n","    model_to_save = model.module if hasattr(model, 'module') else model\n","    model_to_save.save_pretrained(args[\"output_dir\"])\n","    tokenizer.save_pretrained(args[\"output_dir\"])\n","\n","\n","\n","    # Good practice: save your training arguments together with the trained model\n","    torch.save(args, os.path.join(args[\"output_dir\"], 'training_args.bin'))\n","\n","    # Load a trained model and vocabulary that you have fine-tuned\n","    model = model_class.from_pretrained(args[\"output_dir\"])\n","    tokenizer = tokenizer_class.from_pretrained(args[\"output_dir\"], do_lower_case=args[\"do_lower_case\"])\n","    model.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"rSO1GHIJI9l9","outputId":"ed345aca-1981-4046-d5d6-02d050d14e53","executionInfo":{"status":"error","timestamp":1707424244040,"user_tz":-420,"elapsed":370710,"user":{"displayName":"Toàn Nguyễn Văn","userId":"12622235312397842785"}}},"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Training/evaluation parameters %s {'local_rank': -1, 'model_type': 'bert', 'config_name': '', 'model_name_or_path': 'bert-base-multilingual-cased', 'tokenizer_name': '', 'max_seq_length': 384, 'overwrite_cache': False, 'do_lower_case': True, 'do_train': True, 'output_dir': 'models/bert-finetuned-vinews1', 'version_2_with_negative': True, 'doc_stride': 128, 'max_query_length': 64, 'train_file': './ViQuAD1.0/train_ViQuAD.json', 'predict_file': './ViQuAD1.0/dev_ViQuAD.json', 'per_gpu_train_batch_size': 4, 'max_steps': -1, 'num_train_epochs': 3, 'learning_rate': 3e-05, 'adam_epsilon': 1e-08, 'warmup_steps': 100, 'no_cuda': False, 'gradient_accumulation_steps': 1, 'max_grad_norm': 1.0, 'weight_decay': 0.0, 'save_steps': 1000, 'seed': 42, 'do_eval': True, 'eval_all_checkpoints': True, 'eval_batch_size': 8, 'per_gpu_eval_batch_size': 4, 'n_best_size': 20, 'max_answer_length': 300, 'null_score_diff_threshold': 0.0, 'verbose_logging': True, 'n_gpu': 1, 'device': device(type='cuda')}\n","Creating features from dataset file at %s ./ViQuAD1.0/train_ViQuAD.json\n","Could not find answer: '%s' vs. '%s' Là con đầu lòng của Augustine Washington (1694–1743) và người vợ thứ hai, Cha của ông, Augustine là một nhà trồng thuốc lá có sở hữu người nô lệ\n","Could not find answer: '%s' vs. '%s' vụ phân tích mẫu đất bằng phổ kế huỳnh quang tia phân tích mẫu đất bằng phổ kế huỳnh quang tia X\n","*** Example ***\n","unique_id: 1000000000\n","example_index: 0\n","doc_span_index: 0\n","tokens: [CLS] tên gọi nào được phạm văn đồng sử dụng khi làm phó chủ nhiệm cơ quan bi ##ện sự xứ tại qu ##ế lâm ? [SEP] phạm văn đồng ( 1 tháng 3 năm 1906 [UNK] 29 tháng 4 năm 2000 ) là thủ tướng đầu tiên của nước cộng hòa xã hội chủ nghĩa vi ##ệt nam từ năm 1976 ( từ năm 1981 gọi là chủ tịch hội đồng bộ trưởng ) cho đến khi nghỉ h ##ưu năm 1987 . trước đó ông từng giữ chức vụ thủ tướng chính phủ vi ##ệt nam dân chủ cộng hòa từ năm 1955 đến năm 1976 . ông là vị thủ tướng vi ##ệt nam tại vị lâu nhất ( 1955 [UNK] 1987 ) . ông là học trò , cộng sự của chủ tịch hồ chí minh . ông có tên gọi thân mật là t ##ô , đây từng là bí danh của ông . ông còn có tên gọi là lâm bá ki ##ệt khi làm phó chủ nhiệm cơ quan bi ##ện sự xứ tại qu ##ế lâm ( chủ nhiệm là hồ học l ##ã ##m ) . [SEP]\n","token_to_orig_map: 27:0 28:1 29:2 30:3 31:3 32:4 33:5 34:6 35:7 36:8 37:9 38:10 39:11 40:12 41:13 42:13 43:14 44:15 45:16 46:17 47:18 48:19 49:20 50:21 51:22 52:23 53:24 54:25 55:26 56:27 57:27 58:28 59:29 60:30 61:31 62:32 63:32 64:33 65:34 66:35 67:36 68:37 69:38 70:39 71:40 72:41 73:42 74:42 75:43 76:44 77:45 78:46 79:47 80:47 81:48 82:49 83:49 84:50 85:51 86:52 87:53 88:54 89:55 90:56 91:57 92:58 93:59 94:60 95:61 96:61 97:62 98:63 99:64 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:72 108:72 109:73 110:74 111:75 112:76 113:77 114:78 115:78 116:79 117:80 118:81 119:82 120:83 121:84 122:84 123:84 124:84 125:84 126:84 127:85 128:86 129:87 130:88 131:88 132:89 133:90 134:91 135:92 136:93 137:94 138:95 139:96 140:96 141:97 142:98 143:99 144:100 145:101 146:102 147:103 148:104 149:104 150:104 151:105 152:106 153:107 154:108 155:109 156:110 157:111 158:111 159:112 160:113 161:114 162:115 163:116 164:117 165:118 166:119 167:120 168:120 169:121 170:122 171:123 172:124 173:125 174:126 175:127 176:128 177:128 178:129 179:130 180:131 181:132 182:132 183:133 184:134 185:134 186:135 187:136 188:137 189:138 190:139 191:139 192:139 193:139 194:139\n","token_is_max_context: 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True\n","input_ids: 101 15322 17430 27258 10476 50816 19923 15049 14808 15237 12072 12984 84090 16549 22673 16579 12522 11342 32778 12636 65206 12086 10608 26668 89383 136 102 50816 19923 15049 113 122 11642 124 10558 11932 100 10386 11642 125 10558 10180 114 10331 18755 22313 11201 11811 10447 12932 30876 23985 13388 16425 16549 20792 13956 30097 14441 11840 10558 10705 113 11840 10558 10654 17430 10331 16549 41946 16425 15049 13848 22380 114 11257 12002 12072 103387 176 94844 10558 10581 119 16325 12393 12660 26258 25618 16244 17790 18755 22313 12707 21961 13956 30097 14441 12486 16549 30876 23985 11840 10558 11071 12002 10558 10705 119 12660 10331 15598 18755 22313 13956 30097 14441 12086 15598 46281 13346 113 11071 100 10581 114 119 12660 10331 11125 32457 117 30876 12636 10447 16549 41946 50104 33966 30407 119 12660 10601 15322 17430 22894 32263 10331 188 16218 117 20502 26258 10331 57696 22254 10447 12660 119 12660 14674 10601 15322 17430 10331 89383 74686 10879 30097 12072 12984 84090 16549 22673 16579 12522 11342 32778 12636 65206 12086 10608 26668 89383 113 16549 22673 10331 50104 11125 180 18001 10147 114 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 165\n","end_position: 168\n","answer: lâm bá ki ##ệt\n","*** Example ***\n","unique_id: 1000000001\n","example_index: 1\n","doc_span_index: 0\n","tokens: [CLS] phạm văn đồng giữ chức vụ gì trong bộ máy nhà nước cộng hòa xã hội chủ nghĩa vi ##ệt nam ? [SEP] phạm văn đồng ( 1 tháng 3 năm 1906 [UNK] 29 tháng 4 năm 2000 ) là thủ tướng đầu tiên của nước cộng hòa xã hội chủ nghĩa vi ##ệt nam từ năm 1976 ( từ năm 1981 gọi là chủ tịch hội đồng bộ trưởng ) cho đến khi nghỉ h ##ưu năm 1987 . trước đó ông từng giữ chức vụ thủ tướng chính phủ vi ##ệt nam dân chủ cộng hòa từ năm 1955 đến năm 1976 . ông là vị thủ tướng vi ##ệt nam tại vị lâu nhất ( 1955 [UNK] 1987 ) . ông là học trò , cộng sự của chủ tịch hồ chí minh . ông có tên gọi thân mật là t ##ô , đây từng là bí danh của ông . ông còn có tên gọi là lâm bá ki ##ệt khi làm phó chủ nhiệm cơ quan bi ##ện sự xứ tại qu ##ế lâm ( chủ nhiệm là hồ học l ##ã ##m ) . [SEP]\n","token_to_orig_map: 24:0 25:1 26:2 27:3 28:3 29:4 30:5 31:6 32:7 33:8 34:9 35:10 36:11 37:12 38:13 39:13 40:14 41:15 42:16 43:17 44:18 45:19 46:20 47:21 48:22 49:23 50:24 51:25 52:26 53:27 54:27 55:28 56:29 57:30 58:31 59:32 60:32 61:33 62:34 63:35 64:36 65:37 66:38 67:39 68:40 69:41 70:42 71:42 72:43 73:44 74:45 75:46 76:47 77:47 78:48 79:49 80:49 81:50 82:51 83:52 84:53 85:54 86:55 87:56 88:57 89:58 90:59 91:60 92:61 93:61 94:62 95:63 96:64 97:65 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:72 106:73 107:74 108:75 109:76 110:77 111:78 112:78 113:79 114:80 115:81 116:82 117:83 118:84 119:84 120:84 121:84 122:84 123:84 124:85 125:86 126:87 127:88 128:88 129:89 130:90 131:91 132:92 133:93 134:94 135:95 136:96 137:96 138:97 139:98 140:99 141:100 142:101 143:102 144:103 145:104 146:104 147:104 148:105 149:106 150:107 151:108 152:109 153:110 154:111 155:111 156:112 157:113 158:114 159:115 160:116 161:117 162:118 163:119 164:120 165:120 166:121 167:122 168:123 169:124 170:125 171:126 172:127 173:128 174:128 175:129 176:130 177:131 178:132 179:132 180:133 181:134 182:134 183:135 184:136 185:137 186:138 187:139 188:139 189:139 190:139 191:139\n","token_is_max_context: 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True\n","input_ids: 101 50816 19923 15049 25618 16244 17790 49309 10504 13848 18838 13265 12932 30876 23985 13388 16425 16549 20792 13956 30097 14441 136 102 50816 19923 15049 113 122 11642 124 10558 11932 100 10386 11642 125 10558 10180 114 10331 18755 22313 11201 11811 10447 12932 30876 23985 13388 16425 16549 20792 13956 30097 14441 11840 10558 10705 113 11840 10558 10654 17430 10331 16549 41946 16425 15049 13848 22380 114 11257 12002 12072 103387 176 94844 10558 10581 119 16325 12393 12660 26258 25618 16244 17790 18755 22313 12707 21961 13956 30097 14441 12486 16549 30876 23985 11840 10558 11071 12002 10558 10705 119 12660 10331 15598 18755 22313 13956 30097 14441 12086 15598 46281 13346 113 11071 100 10581 114 119 12660 10331 11125 32457 117 30876 12636 10447 16549 41946 50104 33966 30407 119 12660 10601 15322 17430 22894 32263 10331 188 16218 117 20502 26258 10331 57696 22254 10447 12660 119 12660 14674 10601 15322 17430 10331 89383 74686 10879 30097 12072 12984 84090 16549 22673 16579 12522 11342 32778 12636 65206 12086 10608 26668 89383 113 16549 22673 10331 50104 11125 180 18001 10147 114 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 41\n","end_position: 42\n","answer: thủ tướng\n","*** Example ***\n","unique_id: 1000000002\n","example_index: 2\n","doc_span_index: 0\n","tokens: [CLS] giai đoạn năm 1955 - 1976 , phạm văn đồng nắm giữ chức vụ gì ? [SEP] phạm văn đồng ( 1 tháng 3 năm 1906 [UNK] 29 tháng 4 năm 2000 ) là thủ tướng đầu tiên của nước cộng hòa xã hội chủ nghĩa vi ##ệt nam từ năm 1976 ( từ năm 1981 gọi là chủ tịch hội đồng bộ trưởng ) cho đến khi nghỉ h ##ưu năm 1987 . trước đó ông từng giữ chức vụ thủ tướng chính phủ vi ##ệt nam dân chủ cộng hòa từ năm 1955 đến năm 1976 . ông là vị thủ tướng vi ##ệt nam tại vị lâu nhất ( 1955 [UNK] 1987 ) . ông là học trò , cộng sự của chủ tịch hồ chí minh . ông có tên gọi thân mật là t ##ô , đây từng là bí danh của ông . ông còn có tên gọi là lâm bá ki ##ệt khi làm phó chủ nhiệm cơ quan bi ##ện sự xứ tại qu ##ế lâm ( chủ nhiệm là hồ học l ##ã ##m ) . [SEP]\n","token_to_orig_map: 18:0 19:1 20:2 21:3 22:3 23:4 24:5 25:6 26:7 27:8 28:9 29:10 30:11 31:12 32:13 33:13 34:14 35:15 36:16 37:17 38:18 39:19 40:20 41:21 42:22 43:23 44:24 45:25 46:26 47:27 48:27 49:28 50:29 51:30 52:31 53:32 54:32 55:33 56:34 57:35 58:36 59:37 60:38 61:39 62:40 63:41 64:42 65:42 66:43 67:44 68:45 69:46 70:47 71:47 72:48 73:49 74:49 75:50 76:51 77:52 78:53 79:54 80:55 81:56 82:57 83:58 84:59 85:60 86:61 87:61 88:62 89:63 90:64 91:65 92:66 93:67 94:68 95:69 96:70 97:71 98:72 99:72 100:73 101:74 102:75 103:76 104:77 105:78 106:78 107:79 108:80 109:81 110:82 111:83 112:84 113:84 114:84 115:84 116:84 117:84 118:85 119:86 120:87 121:88 122:88 123:89 124:90 125:91 126:92 127:93 128:94 129:95 130:96 131:96 132:97 133:98 134:99 135:100 136:101 137:102 138:103 139:104 140:104 141:104 142:105 143:106 144:107 145:108 146:109 147:110 148:111 149:111 150:112 151:113 152:114 153:115 154:116 155:117 156:118 157:119 158:120 159:120 160:121 161:122 162:123 163:124 164:125 165:126 166:127 167:128 168:128 169:129 170:130 171:131 172:132 173:132 174:133 175:134 176:134 177:135 178:136 179:137 180:138 181:139 182:139 183:139 184:139 185:139\n","token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True\n","input_ids: 101 45549 31709 10558 11071 118 10705 117 50816 19923 15049 94782 25618 16244 17790 49309 136 102 50816 19923 15049 113 122 11642 124 10558 11932 100 10386 11642 125 10558 10180 114 10331 18755 22313 11201 11811 10447 12932 30876 23985 13388 16425 16549 20792 13956 30097 14441 11840 10558 10705 113 11840 10558 10654 17430 10331 16549 41946 16425 15049 13848 22380 114 11257 12002 12072 103387 176 94844 10558 10581 119 16325 12393 12660 26258 25618 16244 17790 18755 22313 12707 21961 13956 30097 14441 12486 16549 30876 23985 11840 10558 11071 12002 10558 10705 119 12660 10331 15598 18755 22313 13956 30097 14441 12086 15598 46281 13346 113 11071 100 10581 114 119 12660 10331 11125 32457 117 30876 12636 10447 16549 41946 50104 33966 30407 119 12660 10601 15322 17430 22894 32263 10331 188 16218 117 20502 26258 10331 57696 22254 10447 12660 119 12660 14674 10601 15322 17430 10331 89383 74686 10879 30097 12072 12984 84090 16549 22673 16579 12522 11342 32778 12636 65206 12086 10608 26668 89383 113 16549 22673 10331 50104 11125 180 18001 10147 114 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 82\n","end_position: 92\n","answer: thủ tướng chính phủ vi ##ệt nam dân chủ cộng hòa\n","*** Example ***\n","unique_id: 1000000003\n","example_index: 3\n","doc_span_index: 0\n","tokens: [CLS] sự kiện quan trọng nào đã diễn ra vào ngày 20 / 7 / 1954 ? [SEP] năm 1954 , ông được giao nhiệm vụ trưởng phái đoàn chính phủ dự hội nghị gen ##ève về đông dương . những đóng góp của đoàn vi ##ệt nam do ông đứng đầu là vô cùng quan trọng , tạo ra những đột phá đưa hội nghị tới thành công . trải qua 8 phiên họp toàn thể và 23 phiên họp rất căn ##g thẳng và ph ##ức tạp , với tinh thần chủ động và cố gắng của phái đoàn vi ##ệt nam , ngày 20 / 7 / 1954 , bản hiệp định đình chỉ chiến sự ở vi ##ệt nam , campu ##chia và là ##o đã được ký kết thừa nhận tôn trọng độc lập , chủ quyền , của nước vi ##ệt nam , là ##o và campu ##chia . [SEP]\n","token_to_orig_map: 18:0 19:1 20:1 21:2 22:3 23:4 24:5 25:6 26:7 27:8 28:9 29:10 30:11 31:12 32:13 33:14 34:15 35:15 36:16 37:17 38:18 39:18 40:19 41:20 42:21 43:22 44:23 45:24 46:24 47:25 48:26 49:27 50:28 51:29 52:30 53:31 54:32 55:33 56:34 57:34 58:35 59:36 60:37 61:38 62:39 63:40 64:41 65:42 66:43 67:44 68:45 69:45 70:46 71:47 72:48 73:49 74:50 75:51 76:52 77:53 78:54 79:55 80:56 81:57 82:58 83:58 84:59 85:60 86:61 87:61 88:62 89:62 90:63 91:64 92:65 93:66 94:67 95:68 96:69 97:70 98:71 99:72 100:73 101:74 102:74 103:75 104:75 105:76 106:77 107:77 108:77 109:77 110:77 111:77 112:78 113:79 114:80 115:81 116:82 117:83 118:84 119:85 120:86 121:86 122:87 123:87 124:88 125:88 126:89 127:90 128:90 129:91 130:92 131:93 132:94 133:95 134:96 135:97 136:98 137:99 138:100 139:100 140:101 141:102 142:102 143:103 144:104 145:105 146:105 147:106 148:106 149:107 150:107 151:108 152:109 153:109 154:109\n","token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True\n","input_ids: 101 12636 34018 12522 22728 27258 11213 17300 11859 11603 12137 10197 120 128 120 11032 136 102 10558 11032 117 12660 10476 25216 22673 17790 22380 39920 16449 12707 21961 24467 16425 40352 15331 32463 12420 23164 30180 119 11934 26885 64648 10447 16449 13956 30097 14441 10149 12660 42123 11201 10331 28443 14557 12522 22728 117 17783 11859 11934 70395 32015 25576 16425 40352 18700 11629 12319 119 86032 14517 129 40548 100727 20005 12334 10432 10328 40548 100727 18946 60741 10240 82766 10432 99142 39909 62861 117 11182 24009 26082 16549 13254 10432 37452 103824 10447 39920 16449 13956 30097 14441 117 12137 10197 120 128 120 11032 117 15194 66866 15027 25309 14294 14463 12636 1682 13956 30097 14441 117 72050 27553 10432 10331 10133 11213 10476 30906 16886 69721 16638 50522 22728 32163 15707 117 16549 17992 117 10447 12932 13956 30097 14441 117 10331 10133 10432 72050 27553 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 112\n","end_position: 153\n","answer: bản hiệp định đình chỉ chiến sự ở vi ##ệt nam , campu ##chia và là ##o đã được ký kết thừa nhận tôn trọng độc lập , chủ quyền , của nước vi ##ệt nam , là ##o và campu ##chia\n","*** Example ***\n","unique_id: 1000000004\n","example_index: 4\n","doc_span_index: 0\n","tokens: [CLS] chức vụ mà phạm văn đồng đảm nhiệm tại hội nghị gen ##ève về đông dương ? [SEP] năm 1954 , ông được giao nhiệm vụ trưởng phái đoàn chính phủ dự hội nghị gen ##ève về đông dương . những đóng góp của đoàn vi ##ệt nam do ông đứng đầu là vô cùng quan trọng , tạo ra những đột phá đưa hội nghị tới thành công . trải qua 8 phiên họp toàn thể và 23 phiên họp rất căn ##g thẳng và ph ##ức tạp , với tinh thần chủ động và cố gắng của phái đoàn vi ##ệt nam , ngày 20 / 7 / 1954 , bản hiệp định đình chỉ chiến sự ở vi ##ệt nam , campu ##chia và là ##o đã được ký kết thừa nhận tôn trọng độc lập , chủ quyền , của nước vi ##ệt nam , là ##o và campu ##chia . [SEP]\n","token_to_orig_map: 19:0 20:1 21:1 22:2 23:3 24:4 25:5 26:6 27:7 28:8 29:9 30:10 31:11 32:12 33:13 34:14 35:15 36:15 37:16 38:17 39:18 40:18 41:19 42:20 43:21 44:22 45:23 46:24 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:31 55:32 56:33 57:34 58:34 59:35 60:36 61:37 62:38 63:39 64:40 65:41 66:42 67:43 68:44 69:45 70:45 71:46 72:47 73:48 74:49 75:50 76:51 77:52 78:53 79:54 80:55 81:56 82:57 83:58 84:58 85:59 86:60 87:61 88:61 89:62 90:62 91:63 92:64 93:65 94:66 95:67 96:68 97:69 98:70 99:71 100:72 101:73 102:74 103:74 104:75 105:75 106:76 107:77 108:77 109:77 110:77 111:77 112:77 113:78 114:79 115:80 116:81 117:82 118:83 119:84 120:85 121:86 122:86 123:87 124:87 125:88 126:88 127:89 128:90 129:90 130:91 131:92 132:93 133:94 134:95 135:96 136:97 137:98 138:99 139:100 140:100 141:101 142:102 143:102 144:103 145:104 146:105 147:105 148:106 149:106 150:107 151:107 152:108 153:109 154:109 155:109\n","token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True\n","input_ids: 101 16244 17790 15542 50816 19923 15049 62787 22673 12086 16425 40352 15331 32463 12420 23164 30180 136 102 10558 11032 117 12660 10476 25216 22673 17790 22380 39920 16449 12707 21961 24467 16425 40352 15331 32463 12420 23164 30180 119 11934 26885 64648 10447 16449 13956 30097 14441 10149 12660 42123 11201 10331 28443 14557 12522 22728 117 17783 11859 11934 70395 32015 25576 16425 40352 18700 11629 12319 119 86032 14517 129 40548 100727 20005 12334 10432 10328 40548 100727 18946 60741 10240 82766 10432 99142 39909 62861 117 11182 24009 26082 16549 13254 10432 37452 103824 10447 39920 16449 13956 30097 14441 117 12137 10197 120 128 120 11032 117 15194 66866 15027 25309 14294 14463 12636 1682 13956 30097 14441 117 72050 27553 10432 10331 10133 11213 10476 30906 16886 69721 16638 50522 22728 32163 15707 117 16549 17992 117 10447 12932 13956 30097 14441 117 10331 10133 10432 72050 27553 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 27\n","end_position: 31\n","answer: trưởng phái đoàn chính phủ\n","*** Example ***\n","unique_id: 1000000005\n","example_index: 5\n","doc_span_index: 0\n","tokens: [CLS] hội nghị gen ##ève về đông dương có tính chất như thế nào ? [SEP] năm 1954 , ông được giao nhiệm vụ trưởng phái đoàn chính phủ dự hội nghị gen ##ève về đông dương . những đóng góp của đoàn vi ##ệt nam do ông đứng đầu là vô cùng quan trọng , tạo ra những đột phá đưa hội nghị tới thành công . trải qua 8 phiên họp toàn thể và 23 phiên họp rất căn ##g thẳng và ph ##ức tạp , với tinh thần chủ động và cố gắng của phái đoàn vi ##ệt nam , ngày 20 / 7 / 1954 , bản hiệp định đình chỉ chiến sự ở vi ##ệt nam , campu ##chia và là ##o đã được ký kết thừa nhận tôn trọng độc lập , chủ quyền , của nước vi ##ệt nam , là ##o và campu ##chia . [SEP]\n","token_to_orig_map: 16:0 17:1 18:1 19:2 20:3 21:4 22:5 23:6 24:7 25:8 26:9 27:10 28:11 29:12 30:13 31:14 32:15 33:15 34:16 35:17 36:18 37:18 38:19 39:20 40:21 41:22 42:23 43:24 44:24 45:25 46:26 47:27 48:28 49:29 50:30 51:31 52:32 53:33 54:34 55:34 56:35 57:36 58:37 59:38 60:39 61:40 62:41 63:42 64:43 65:44 66:45 67:45 68:46 69:47 70:48 71:49 72:50 73:51 74:52 75:53 76:54 77:55 78:56 79:57 80:58 81:58 82:59 83:60 84:61 85:61 86:62 87:62 88:63 89:64 90:65 91:66 92:67 93:68 94:69 95:70 96:71 97:72 98:73 99:74 100:74 101:75 102:75 103:76 104:77 105:77 106:77 107:77 108:77 109:77 110:78 111:79 112:80 113:81 114:82 115:83 116:84 117:85 118:86 119:86 120:87 121:87 122:88 123:88 124:89 125:90 126:90 127:91 128:92 129:93 130:94 131:95 132:96 133:97 134:98 135:99 136:100 137:100 138:101 139:102 140:102 141:103 142:104 143:105 144:105 145:106 146:106 147:107 148:107 149:108 150:109 151:109 152:109\n","token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True\n","input_ids: 101 16425 40352 15331 32463 12420 23164 30180 10601 17739 21913 12552 14421 27258 136 102 10558 11032 117 12660 10476 25216 22673 17790 22380 39920 16449 12707 21961 24467 16425 40352 15331 32463 12420 23164 30180 119 11934 26885 64648 10447 16449 13956 30097 14441 10149 12660 42123 11201 10331 28443 14557 12522 22728 117 17783 11859 11934 70395 32015 25576 16425 40352 18700 11629 12319 119 86032 14517 129 40548 100727 20005 12334 10432 10328 40548 100727 18946 60741 10240 82766 10432 99142 39909 62861 117 11182 24009 26082 16549 13254 10432 37452 103824 10447 39920 16449 13956 30097 14441 117 12137 10197 120 128 120 11032 117 15194 66866 15027 25309 14294 14463 12636 1682 13956 30097 14441 117 72050 27553 10432 10331 10133 11213 10476 30906 16886 69721 16638 50522 22728 32163 15707 117 16549 17992 117 10447 12932 13956 30097 14441 117 10331 10133 10432 72050 27553 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 79\n","end_position: 86\n","answer: rất căn ##g thẳng và ph ##ức tạp\n","*** Example ***\n","unique_id: 1000000006\n","example_index: 6\n","doc_span_index: 0\n","tokens: [CLS] phạm sơ ##n dương , con trai của phạm văn đồng , đang giữ chức vụ gì ? [SEP] phạm văn đồng có vợ là bà phạm thị c ##úc và một người con trai duy nhất tên là phạm sơ ##n dương , hiện là thiếu tướng quân đội nhân dân vi ##ệt nam , phó giám đốc viện khoa học và công nghệ quân sự . sau khi lấy bà c ##úc ( tháng 10 năm 1946 ) phạm văn đồng vào công tác trong liên khu 5 . m ##ấy năm sau bà c ##úc được phép vào nam sống với chồng . vào đến nơi thì phạm văn đồng lại được lệnh ra bắc . sau đó bà c ##úc bị bệnh \" nửa quê ##n nửa nhớ \" ( theo lời của vi ##ệt phương , người từng làm thư ký cho phạm văn đồng trong 53 năm ) kéo dài cho đến tận b ##ây giờ . phạm văn đồng từng đưa bà c ##úc sang trung quốc , liên x ##ô chữa bệnh nhưng vẫn không khỏi . phạm văn đồng mất trước bà c ##úc . [SEP]\n","token_to_orig_map: 20:0 21:1 22:2 23:3 24:4 25:5 26:6 27:7 28:8 29:9 30:9 31:10 32:11 33:12 34:13 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:20 43:21 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:31 57:32 58:33 59:34 60:35 61:36 62:37 63:38 64:39 65:40 66:41 67:42 68:42 69:43 70:44 71:45 72:46 73:47 74:47 75:48 76:48 77:49 78:50 79:51 80:51 81:52 82:53 83:54 84:55 85:56 86:57 87:58 88:59 89:60 90:61 91:61 92:62 93:62 94:63 95:64 96:65 97:66 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:73 106:73 107:74 108:75 109:76 110:77 111:78 112:79 113:80 114:81 115:82 116:83 117:84 118:85 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:92 129:93 130:93 131:94 132:95 133:95 134:96 135:96 136:97 137:98 138:99 139:99 140:100 141:100 142:101 143:102 144:103 145:104 146:105 147:106 148:107 149:108 150:109 151:110 152:111 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:118 162:119 163:119 164:120 165:121 166:122 167:123 168:124 169:125 170:126 171:126 172:127 173:128 174:129 175:129 176:130 177:131 178:131 179:132 180:133 181:134 182:135 183:136 184:137 185:137 186:138 187:139 188:140 189:141 190:142 191:143 192:144 193:144 194:144\n","token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True\n","input_ids: 101 50816 81128 10115 30180 117 10173 34101 10447 50816 19923 15049 117 21080 25618 16244 17790 49309 136 102 50816 19923 15049 10601 64264 10331 27083 50816 14373 171 42596 10432 10417 11027 10173 34101 38779 13346 15322 10331 50816 81128 10115 30180 117 13526 10331 54594 22313 12488 15671 14694 12486 13956 30097 14441 117 84090 45642 45529 27805 11685 11125 10432 12319 28267 12488 12636 119 11731 12072 29937 27083 171 42596 113 11642 10150 10558 11063 114 50816 19923 15049 11603 12319 17976 10504 20419 16028 126 119 181 48215 10558 11731 27083 171 42596 10476 34284 11603 14441 17135 11182 88652 119 11603 12002 21670 18004 50816 19923 15049 13148 10476 33586 11859 23088 119 11731 12393 27083 171 42596 12505 32686 107 83544 86802 10115 83544 90290 107 113 13951 34619 10447 13956 30097 20377 117 11027 26258 12984 30355 30906 11257 50816 19923 15049 10504 11756 10558 114 47598 21443 11257 12002 109327 170 43616 29656 119 50816 19923 15049 26258 25576 27083 171 42596 15208 16396 14369 117 20419 192 16218 100222 32686 15662 22666 11755 33718 119 50816 19923 15049 28022 16325 27083 171 42596 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 47\n","end_position: 67\n","answer: thiếu tướng quân đội nhân dân vi ##ệt nam , phó giám đốc viện khoa học và công nghệ quân sự\n","*** Example ***\n","unique_id: 1000000007\n","example_index: 7\n","doc_span_index: 0\n","tokens: [CLS] phạm văn đồng từng cố gắng đưa bà c ##úc đến nơi nào để chữa bệnh ? [SEP] phạm văn đồng có vợ là bà phạm thị c ##úc và một người con trai duy nhất tên là phạm sơ ##n dương , hiện là thiếu tướng quân đội nhân dân vi ##ệt nam , phó giám đốc viện khoa học và công nghệ quân sự . sau khi lấy bà c ##úc ( tháng 10 năm 1946 ) phạm văn đồng vào công tác trong liên khu 5 . m ##ấy năm sau bà c ##úc được phép vào nam sống với chồng . vào đến nơi thì phạm văn đồng lại được lệnh ra bắc . sau đó bà c ##úc bị bệnh \" nửa quê ##n nửa nhớ \" ( theo lời của vi ##ệt phương , người từng làm thư ký cho phạm văn đồng trong 53 năm ) kéo dài cho đến tận b ##ây giờ . phạm văn đồng từng đưa bà c ##úc sang trung quốc , liên x ##ô chữa bệnh nhưng vẫn không khỏi . phạm văn đồng mất trước bà c ##úc . [SEP]\n","token_to_orig_map: 19:0 20:1 21:2 22:3 23:4 24:5 25:6 26:7 27:8 28:9 29:9 30:10 31:11 32:12 33:13 34:14 35:15 36:16 37:17 38:18 39:19 40:20 41:20 42:21 43:21 44:22 45:23 46:24 47:25 48:26 49:27 50:28 51:29 52:30 53:30 54:31 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:40 65:41 66:42 67:42 68:43 69:44 70:45 71:46 72:47 73:47 74:48 75:48 76:49 77:50 78:51 79:51 80:52 81:53 82:54 83:55 84:56 85:57 86:58 87:59 88:60 89:61 90:61 91:62 92:62 93:63 94:64 95:65 96:66 97:66 98:67 99:68 100:69 101:70 102:71 103:72 104:73 105:73 106:74 107:75 108:76 109:77 110:78 111:79 112:80 113:81 114:82 115:83 116:84 117:85 118:85 119:86 120:87 121:88 122:89 123:89 124:90 125:91 126:92 127:92 128:93 129:93 130:94 131:95 132:95 133:96 134:96 135:97 136:98 137:99 138:99 139:100 140:100 141:101 142:102 143:103 144:104 145:105 146:106 147:107 148:108 149:109 150:110 151:111 152:112 153:112 154:113 155:114 156:115 157:116 158:117 159:118 160:118 161:119 162:119 163:120 164:121 165:122 166:123 167:124 168:125 169:126 170:126 171:127 172:128 173:129 174:129 175:130 176:131 177:131 178:132 179:133 180:134 181:135 182:136 183:137 184:137 185:138 186:139 187:140 188:141 189:142 190:143 191:144 192:144 193:144\n","token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True\n","input_ids: 101 50816 19923 15049 26258 37452 103824 25576 27083 171 42596 12002 21670 27258 12460 100222 32686 136 102 50816 19923 15049 10601 64264 10331 27083 50816 14373 171 42596 10432 10417 11027 10173 34101 38779 13346 15322 10331 50816 81128 10115 30180 117 13526 10331 54594 22313 12488 15671 14694 12486 13956 30097 14441 117 84090 45642 45529 27805 11685 11125 10432 12319 28267 12488 12636 119 11731 12072 29937 27083 171 42596 113 11642 10150 10558 11063 114 50816 19923 15049 11603 12319 17976 10504 20419 16028 126 119 181 48215 10558 11731 27083 171 42596 10476 34284 11603 14441 17135 11182 88652 119 11603 12002 21670 18004 50816 19923 15049 13148 10476 33586 11859 23088 119 11731 12393 27083 171 42596 12505 32686 107 83544 86802 10115 83544 90290 107 113 13951 34619 10447 13956 30097 20377 117 11027 26258 12984 30355 30906 11257 50816 19923 15049 10504 11756 10558 114 47598 21443 11257 12002 109327 170 43616 29656 119 50816 19923 15049 26258 25576 27083 171 42596 15208 16396 14369 117 20419 192 16218 100222 32686 15662 22666 11755 33718 119 50816 19923 15049 28022 16325 27083 171 42596 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 172\n","end_position: 177\n","answer: trung quốc , liên x ##ô\n","*** Example ***\n","unique_id: 1000000008\n","example_index: 8\n","doc_span_index: 0\n","tokens: [CLS] chứng bệnh mà bà c ##úc m ##ắc phải đến tận b ##ây gi ##ò là gì ? [SEP] phạm văn đồng có vợ là bà phạm thị c ##úc và một người con trai duy nhất tên là phạm sơ ##n dương , hiện là thiếu tướng quân đội nhân dân vi ##ệt nam , phó giám đốc viện khoa học và công nghệ quân sự . sau khi lấy bà c ##úc ( tháng 10 năm 1946 ) phạm văn đồng vào công tác trong liên khu 5 . m ##ấy năm sau bà c ##úc được phép vào nam sống với chồng . vào đến nơi thì phạm văn đồng lại được lệnh ra bắc . sau đó bà c ##úc bị bệnh \" nửa quê ##n nửa nhớ \" ( theo lời của vi ##ệt phương , người từng làm thư ký cho phạm văn đồng trong 53 năm ) kéo dài cho đến tận b ##ây giờ . phạm văn đồng từng đưa bà c ##úc sang trung quốc , liên x ##ô chữa bệnh nhưng vẫn không khỏi . phạm văn đồng mất trước bà c ##úc . [SEP]\n","token_to_orig_map: 20:0 21:1 22:2 23:3 24:4 25:5 26:6 27:7 28:8 29:9 30:9 31:10 32:11 33:12 34:13 35:14 36:15 37:16 38:17 39:18 40:19 41:20 42:20 43:21 44:21 45:22 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:30 54:30 55:31 56:31 57:32 58:33 59:34 60:35 61:36 62:37 63:38 64:39 65:40 66:41 67:42 68:42 69:43 70:44 71:45 72:46 73:47 74:47 75:48 76:48 77:49 78:50 79:51 80:51 81:52 82:53 83:54 84:55 85:56 86:57 87:58 88:59 89:60 90:61 91:61 92:62 93:62 94:63 95:64 96:65 97:66 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:73 106:73 107:74 108:75 109:76 110:77 111:78 112:79 113:80 114:81 115:82 116:83 117:84 118:85 119:85 120:86 121:87 122:88 123:89 124:89 125:90 126:91 127:92 128:92 129:93 130:93 131:94 132:95 133:95 134:96 135:96 136:97 137:98 138:99 139:99 140:100 141:100 142:101 143:102 144:103 145:104 146:105 147:106 148:107 149:108 150:109 151:110 152:111 153:112 154:112 155:113 156:114 157:115 158:116 159:117 160:118 161:118 162:119 163:119 164:120 165:121 166:122 167:123 168:124 169:125 170:126 171:126 172:127 173:128 174:129 175:129 176:130 177:131 178:131 179:132 180:133 181:134 182:135 183:136 184:137 185:137 186:138 187:139 188:140 189:141 190:142 191:143 192:144 193:144 194:144\n","token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True\n","input_ids: 101 34445 32686 15542 27083 171 42596 181 36228 15723 12002 109327 170 43616 38356 11243 10331 49309 136 102 50816 19923 15049 10601 64264 10331 27083 50816 14373 171 42596 10432 10417 11027 10173 34101 38779 13346 15322 10331 50816 81128 10115 30180 117 13526 10331 54594 22313 12488 15671 14694 12486 13956 30097 14441 117 84090 45642 45529 27805 11685 11125 10432 12319 28267 12488 12636 119 11731 12072 29937 27083 171 42596 113 11642 10150 10558 11063 114 50816 19923 15049 11603 12319 17976 10504 20419 16028 126 119 181 48215 10558 11731 27083 171 42596 10476 34284 11603 14441 17135 11182 88652 119 11603 12002 21670 18004 50816 19923 15049 13148 10476 33586 11859 23088 119 11731 12393 27083 171 42596 12505 32686 107 83544 86802 10115 83544 90290 107 113 13951 34619 10447 13956 30097 20377 117 11027 26258 12984 30355 30906 11257 50816 19923 15049 10504 11756 10558 114 47598 21443 11257 12002 109327 170 43616 29656 119 50816 19923 15049 26258 25576 27083 171 42596 15208 16396 14369 117 20419 192 16218 100222 32686 15662 22666 11755 33718 119 50816 19923 15049 28022 16325 27083 171 42596 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 128\n","end_position: 132\n","answer: nửa quê ##n nửa nhớ\n","*** Example ***\n","unique_id: 1000000009\n","example_index: 9\n","doc_span_index: 0\n","tokens: [CLS] sai l ##ầm mà cố thủ tướng phạm văn đồng m ##ắc phải khi đ ##à ##m phán hiệp định gene ##va là gì ? [SEP] ông vi ##ệt phương , nguyên thư ký của thủ tướng phạm văn đồng , trong buổi họp báo giới thiệu sách của các nhà ngoại giao , đã tiết lộ rằng khi đ ##à ##m phán hiệp định gene ##va ( 1954 ) , do đoàn vi ##ệt nam không có điện đài nên bộ trưởng ngoại giao lúc đó là phạm văn đồng đã m ##ắc một sai l ##ầm khi nhờ trung quốc chuyển các bức điện về nước , do vậy trung quốc biết hết các sách lược của vi ##ệt nam và sử dụng chúng để é ##p vi ##ệt nam ký hiệp định theo lợi í ##ch của trung quốc . trong đ ##à ##m phán phạm văn đồng sử dụng phiên dịch trung quốc nên nội dung liên lạc giữa đoàn đ ##à ##m phán và trung ương , trung quốc đều biết trước và tìm cách ngăn chặn . ông phạm văn đồng sau này cũng thừa nhận là đoàn vi ##ệt nam khi đó quá tin đoàn trung quốc . tại hội nghị ấy , ông đồng chỉ chủ yếu tiếp xúc với đoàn liên x ##ô và đoàn trung quốc , trong khi anh là đồng chủ tịch , quan điểm lại khác với pháp , nhưng ông lại không tranh thủ , không h ##ề tiếp xúc với phái đoàn anh . [SEP]\n","token_to_orig_map: 27:0 28:1 29:1 30:2 31:2 32:3 33:4 34:5 35:6 36:7 37:8 38:9 39:10 40:11 41:11 42:12 43:13 44:14 45:15 46:16 47:17 48:18 49:19 50:20 51:21 52:22 53:23 54:23 55:24 56:25 57:26 58:27 59:28 60:29 61:29 62:29 63:30 64:31 65:32 66:33 67:33 68:34 69:34 70:34 71:34 72:35 73:36 74:37 75:37 76:38 77:39 78:40 79:41 80:42 81:43 82:44 83:45 84:46 85:47 86:48 87:49 88:50 89:51 90:52 91:53 92:54 93:55 94:55 95:56 96:57 97:58 98:58 99:59 100:60 101:61 102:62 103:63 104:64 105:65 106:66 107:67 108:68 109:68 110:69 111:70 112:71 113:72 114:73 115:74 116:75 117:76 118:77 119:78 120:79 121:79 122:80 123:81 124:82 125:83 126:84 127:85 128:86 129:86 130:87 131:87 132:88 133:89 134:90 135:91 136:92 137:93 138:94 139:94 140:95 141:96 142:97 143:97 144:98 145:99 146:99 147:99 148:100 149:101 150:102 151:103 152:104 153:105 154:106 155:107 156:108 157:109 158:110 159:111 160:112 161:113 162:114 163:115 164:116 165:117 166:117 167:117 168:118 169:119 170:120 171:121 172:121 173:122 174:123 175:124 176:125 177:126 178:127 179:128 180:129 181:130 182:131 183:131 184:132 185:133 186:134 187:135 188:136 189:137 190:138 191:139 192:140 193:141 194:142 195:143 196:143 197:144 198:145 199:146 200:147 201:148 202:149 203:150 204:151 205:151 206:152 207:153 208:154 209:155 210:155 211:156 212:157 213:158 214:159 215:160 216:161 217:162 218:163 219:164 220:165 221:166 222:166 223:167 224:168 225:169 226:170 227:170 228:171 229:172 230:173 231:174 232:175 233:176 234:177 235:177 236:178 237:179 238:180 239:181 240:182 241:183 242:183 243:184 244:185 245:186 246:187 247:188 248:189 249:189 250:190 251:191 252:191 253:192 254:193 255:194 256:195 257:196 258:197 259:197\n","token_is_max_context: 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True\n","input_ids: 101 13410 180 31985 15542 37452 18755 22313 50816 19923 15049 181 36228 15723 12072 299 10816 10147 108481 66866 15027 24910 10362 10331 49309 136 102 12660 13956 30097 20377 117 22862 30355 30906 10447 18755 22313 50816 19923 15049 117 10504 69542 100727 24276 15814 105176 24544 10447 10792 13265 37458 25216 117 11213 43419 43993 16487 12072 299 10816 10147 108481 66866 15027 24910 10362 113 11032 114 117 10149 16449 13956 30097 14441 11755 10601 23087 57182 19114 13848 22380 37458 25216 28298 12393 10331 50816 19923 15049 11213 181 36228 10417 13410 180 31985 12072 71038 16396 14369 20383 10792 60127 23087 12420 12932 117 10149 31811 16396 14369 21820 33493 10792 24544 64922 10447 13956 30097 14441 10432 14808 15237 18097 12460 263 10410 13956 30097 14441 30906 66866 15027 13951 38081 267 10269 10447 16396 14369 119 10504 299 10816 10147 108481 50816 19923 15049 14808 15237 40548 22654 16396 14369 19114 35498 50622 20419 34255 20734 16449 299 10816 10147 108481 10432 16396 61697 117 16396 14369 22389 21820 16325 10432 23375 15605 72228 95271 119 12660 50816 19923 15049 11731 10789 13284 69721 16638 10331 16449 13956 30097 14441 12072 12393 27261 21629 16449 16396 14369 119 12086 16425 40352 52256 117 12660 15049 14294 16549 25993 16948 88636 11182 16449 20419 192 16218 10432 16449 16396 14369 117 10504 12072 19087 10331 15049 16549 41946 117 12522 15509 13148 14393 11182 19013 117 15662 12660 13148 11755 19383 18755 117 11755 176 43021 16948 88636 11182 39920 16449 19087 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 100\n","end_position: 108\n","answer: nhờ trung quốc chuyển các bức điện về nước\n","*** Example ***\n","unique_id: 1000000010\n","example_index: 10\n","doc_span_index: 0\n","tokens: [CLS] tại sao việc phạm văn đồng sử dụng phiên dịch người trung quốc tại cuộc đ ##à ##m phán là sai l ##ầm ? [SEP] ông vi ##ệt phương , nguyên thư ký của thủ tướng phạm văn đồng , trong buổi họp báo giới thiệu sách của các nhà ngoại giao , đã tiết lộ rằng khi đ ##à ##m phán hiệp định gene ##va ( 1954 ) , do đoàn vi ##ệt nam không có điện đài nên bộ trưởng ngoại giao lúc đó là phạm văn đồng đã m ##ắc một sai l ##ầm khi nhờ trung quốc chuyển các bức điện về nước , do vậy trung quốc biết hết các sách lược của vi ##ệt nam và sử dụng chúng để é ##p vi ##ệt nam ký hiệp định theo lợi í ##ch của trung quốc . trong đ ##à ##m phán phạm văn đồng sử dụng phiên dịch trung quốc nên nội dung liên lạc giữa đoàn đ ##à ##m phán và trung ương , trung quốc đều biết trước và tìm cách ngăn chặn . ông phạm văn đồng sau này cũng thừa nhận là đoàn vi ##ệt nam khi đó quá tin đoàn trung quốc . tại hội nghị ấy , ông đồng chỉ chủ yếu tiếp xúc với đoàn liên x ##ô và đoàn trung quốc , trong khi anh là đồng chủ tịch , quan điểm lại khác với pháp , nhưng ông lại không tranh thủ , không h ##ề tiếp xúc với phái đoàn anh . [SEP]\n","token_to_orig_map: 26:0 27:1 28:1 29:2 30:2 31:3 32:4 33:5 34:6 35:7 36:8 37:9 38:10 39:11 40:11 41:12 42:13 43:14 44:15 45:16 46:17 47:18 48:19 49:20 50:21 51:22 52:23 53:23 54:24 55:25 56:26 57:27 58:28 59:29 60:29 61:29 62:30 63:31 64:32 65:33 66:33 67:34 68:34 69:34 70:34 71:35 72:36 73:37 74:37 75:38 76:39 77:40 78:41 79:42 80:43 81:44 82:45 83:46 84:47 85:48 86:49 87:50 88:51 89:52 90:53 91:54 92:55 93:55 94:56 95:57 96:58 97:58 98:59 99:60 100:61 101:62 102:63 103:64 104:65 105:66 106:67 107:68 108:68 109:69 110:70 111:71 112:72 113:73 114:74 115:75 116:76 117:77 118:78 119:79 120:79 121:80 122:81 123:82 124:83 125:84 126:85 127:86 128:86 129:87 130:87 131:88 132:89 133:90 134:91 135:92 136:93 137:94 138:94 139:95 140:96 141:97 142:97 143:98 144:99 145:99 146:99 147:100 148:101 149:102 150:103 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:116 164:117 165:117 166:117 167:118 168:119 169:120 170:121 171:121 172:122 173:123 174:124 175:125 176:126 177:127 178:128 179:129 180:130 181:131 182:131 183:132 184:133 185:134 186:135 187:136 188:137 189:138 190:139 191:140 192:141 193:142 194:143 195:143 196:144 197:145 198:146 199:147 200:148 201:149 202:150 203:151 204:151 205:152 206:153 207:154 208:155 209:155 210:156 211:157 212:158 213:159 214:160 215:161 216:162 217:163 218:164 219:165 220:166 221:166 222:167 223:168 224:169 225:170 226:170 227:171 228:172 229:173 230:174 231:175 232:176 233:177 234:177 235:178 236:179 237:180 238:181 239:182 240:183 241:183 242:184 243:185 244:186 245:187 246:188 247:189 248:189 249:190 250:191 251:191 252:192 253:193 254:194 255:195 256:196 257:197 258:197\n","token_is_max_context: 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True\n","input_ids: 101 12086 36993 14331 50816 19923 15049 14808 15237 40548 22654 11027 16396 14369 12086 15625 299 10816 10147 108481 10331 13410 180 31985 136 102 12660 13956 30097 20377 117 22862 30355 30906 10447 18755 22313 50816 19923 15049 117 10504 69542 100727 24276 15814 105176 24544 10447 10792 13265 37458 25216 117 11213 43419 43993 16487 12072 299 10816 10147 108481 66866 15027 24910 10362 113 11032 114 117 10149 16449 13956 30097 14441 11755 10601 23087 57182 19114 13848 22380 37458 25216 28298 12393 10331 50816 19923 15049 11213 181 36228 10417 13410 180 31985 12072 71038 16396 14369 20383 10792 60127 23087 12420 12932 117 10149 31811 16396 14369 21820 33493 10792 24544 64922 10447 13956 30097 14441 10432 14808 15237 18097 12460 263 10410 13956 30097 14441 30906 66866 15027 13951 38081 267 10269 10447 16396 14369 119 10504 299 10816 10147 108481 50816 19923 15049 14808 15237 40548 22654 16396 14369 19114 35498 50622 20419 34255 20734 16449 299 10816 10147 108481 10432 16396 61697 117 16396 14369 22389 21820 16325 10432 23375 15605 72228 95271 119 12660 50816 19923 15049 11731 10789 13284 69721 16638 10331 16449 13956 30097 14441 12072 12393 27261 21629 16449 16396 14369 119 12086 16425 40352 52256 117 12660 15049 14294 16549 25993 16948 88636 11182 16449 20419 192 16218 10432 16449 16396 14369 117 10504 12072 19087 10331 15049 16549 41946 117 12522 15509 13148 14393 11182 19013 117 15662 12660 13148 11755 19383 18755 117 11755 176 43021 16948 88636 11182 39920 16449 19087 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 158\n","end_position: 181\n","answer: nội dung liên lạc giữa đoàn đ ##à ##m phán và trung ương , trung quốc đều biết trước và tìm cách ngăn chặn\n","*** Example ***\n","unique_id: 1000000011\n","example_index: 11\n","doc_span_index: 0\n","tokens: [CLS] nguyên nhân nào dẫn đến các sai l ##ầm mà phạm văn đồng m ##ắc phải ? [SEP] ông vi ##ệt phương , nguyên thư ký của thủ tướng phạm văn đồng , trong buổi họp báo giới thiệu sách của các nhà ngoại giao , đã tiết lộ rằng khi đ ##à ##m phán hiệp định gene ##va ( 1954 ) , do đoàn vi ##ệt nam không có điện đài nên bộ trưởng ngoại giao lúc đó là phạm văn đồng đã m ##ắc một sai l ##ầm khi nhờ trung quốc chuyển các bức điện về nước , do vậy trung quốc biết hết các sách lược của vi ##ệt nam và sử dụng chúng để é ##p vi ##ệt nam ký hiệp định theo lợi í ##ch của trung quốc . trong đ ##à ##m phán phạm văn đồng sử dụng phiên dịch trung quốc nên nội dung liên lạc giữa đoàn đ ##à ##m phán và trung ương , trung quốc đều biết trước và tìm cách ngăn chặn . ông phạm văn đồng sau này cũng thừa nhận là đoàn vi ##ệt nam khi đó quá tin đoàn trung quốc . tại hội nghị ấy , ông đồng chỉ chủ yếu tiếp xúc với đoàn liên x ##ô và đoàn trung quốc , trong khi anh là đồng chủ tịch , quan điểm lại khác với pháp , nhưng ông lại không tranh thủ , không h ##ề tiếp xúc với phái đoàn anh . [SEP]\n","token_to_orig_map: 19:0 20:1 21:1 22:2 23:2 24:3 25:4 26:5 27:6 28:7 29:8 30:9 31:10 32:11 33:11 34:12 35:13 36:14 37:15 38:16 39:17 40:18 41:19 42:20 43:21 44:22 45:23 46:23 47:24 48:25 49:26 50:27 51:28 52:29 53:29 54:29 55:30 56:31 57:32 58:33 59:33 60:34 61:34 62:34 63:34 64:35 65:36 66:37 67:37 68:38 69:39 70:40 71:41 72:42 73:43 74:44 75:45 76:46 77:47 78:48 79:49 80:50 81:51 82:52 83:53 84:54 85:55 86:55 87:56 88:57 89:58 90:58 91:59 92:60 93:61 94:62 95:63 96:64 97:65 98:66 99:67 100:68 101:68 102:69 103:70 104:71 105:72 106:73 107:74 108:75 109:76 110:77 111:78 112:79 113:79 114:80 115:81 116:82 117:83 118:84 119:85 120:86 121:86 122:87 123:87 124:88 125:89 126:90 127:91 128:92 129:93 130:94 131:94 132:95 133:96 134:97 135:97 136:98 137:99 138:99 139:99 140:100 141:101 142:102 143:103 144:104 145:105 146:106 147:107 148:108 149:109 150:110 151:111 152:112 153:113 154:114 155:115 156:116 157:117 158:117 159:117 160:118 161:119 162:120 163:121 164:121 165:122 166:123 167:124 168:125 169:126 170:127 171:128 172:129 173:130 174:131 175:131 176:132 177:133 178:134 179:135 180:136 181:137 182:138 183:139 184:140 185:141 186:142 187:143 188:143 189:144 190:145 191:146 192:147 193:148 194:149 195:150 196:151 197:151 198:152 199:153 200:154 201:155 202:155 203:156 204:157 205:158 206:159 207:160 208:161 209:162 210:163 211:164 212:165 213:166 214:166 215:167 216:168 217:169 218:170 219:170 220:171 221:172 222:173 223:174 224:175 225:176 226:177 227:177 228:178 229:179 230:180 231:181 232:182 233:183 234:183 235:184 236:185 237:186 238:187 239:188 240:189 241:189 242:190 243:191 244:191 245:192 246:193 247:194 248:195 249:196 250:197 251:197\n","token_is_max_context: 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True\n","input_ids: 101 22862 14694 27258 26093 12002 10792 13410 180 31985 15542 50816 19923 15049 181 36228 15723 136 102 12660 13956 30097 20377 117 22862 30355 30906 10447 18755 22313 50816 19923 15049 117 10504 69542 100727 24276 15814 105176 24544 10447 10792 13265 37458 25216 117 11213 43419 43993 16487 12072 299 10816 10147 108481 66866 15027 24910 10362 113 11032 114 117 10149 16449 13956 30097 14441 11755 10601 23087 57182 19114 13848 22380 37458 25216 28298 12393 10331 50816 19923 15049 11213 181 36228 10417 13410 180 31985 12072 71038 16396 14369 20383 10792 60127 23087 12420 12932 117 10149 31811 16396 14369 21820 33493 10792 24544 64922 10447 13956 30097 14441 10432 14808 15237 18097 12460 263 10410 13956 30097 14441 30906 66866 15027 13951 38081 267 10269 10447 16396 14369 119 10504 299 10816 10147 108481 50816 19923 15049 14808 15237 40548 22654 16396 14369 19114 35498 50622 20419 34255 20734 16449 299 10816 10147 108481 10432 16396 61697 117 16396 14369 22389 21820 16325 10432 23375 15605 72228 95271 119 12660 50816 19923 15049 11731 10789 13284 69721 16638 10331 16449 13956 30097 14441 12072 12393 27261 21629 16449 16396 14369 119 12086 16425 40352 52256 117 12660 15049 14294 16549 25993 16948 88636 11182 16449 20419 192 16218 10432 16449 16396 14369 117 10504 12072 19087 10331 15049 16549 41946 117 12522 15509 13148 14393 11182 19013 117 15662 12660 13148 11755 19383 18755 117 11755 176 43021 16948 88636 11182 39920 16449 19087 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 186\n","end_position: 196\n","answer: đoàn vi ##ệt nam khi đó quá tin đoàn trung quốc\n","*** Example ***\n","unique_id: 1000000012\n","example_index: 12\n","doc_span_index: 0\n","tokens: [CLS] từ những sai l ##ầm tại cuộc đ ##à ##m phán hiệp định gene ##va , trung quốc đã tận dụng chúng như thế nào ? [SEP] ông vi ##ệt phương , nguyên thư ký của thủ tướng phạm văn đồng , trong buổi họp báo giới thiệu sách của các nhà ngoại giao , đã tiết lộ rằng khi đ ##à ##m phán hiệp định gene ##va ( 1954 ) , do đoàn vi ##ệt nam không có điện đài nên bộ trưởng ngoại giao lúc đó là phạm văn đồng đã m ##ắc một sai l ##ầm khi nhờ trung quốc chuyển các bức điện về nước , do vậy trung quốc biết hết các sách lược của vi ##ệt nam và sử dụng chúng để é ##p vi ##ệt nam ký hiệp định theo lợi í ##ch của trung quốc . trong đ ##à ##m phán phạm văn đồng sử dụng phiên dịch trung quốc nên nội dung liên lạc giữa đoàn đ ##à ##m phán và trung ương , trung quốc đều biết trước và tìm cách ngăn chặn . ông phạm văn đồng sau này cũng thừa nhận là đoàn vi ##ệt nam khi đó quá tin đoàn trung quốc . tại hội nghị ấy , ông đồng chỉ chủ yếu tiếp xúc với đoàn liên x ##ô và đoàn trung quốc , trong khi anh là đồng chủ tịch , quan điểm lại khác với pháp , nhưng ông lại không tranh thủ , không h ##ề tiếp xúc với phái đoàn anh . [SEP]\n","token_to_orig_map: 28:0 29:1 30:1 31:2 32:2 33:3 34:4 35:5 36:6 37:7 38:8 39:9 40:10 41:11 42:11 43:12 44:13 45:14 46:15 47:16 48:17 49:18 50:19 51:20 52:21 53:22 54:23 55:23 56:24 57:25 58:26 59:27 60:28 61:29 62:29 63:29 64:30 65:31 66:32 67:33 68:33 69:34 70:34 71:34 72:34 73:35 74:36 75:37 76:37 77:38 78:39 79:40 80:41 81:42 82:43 83:44 84:45 85:46 86:47 87:48 88:49 89:50 90:51 91:52 92:53 93:54 94:55 95:55 96:56 97:57 98:58 99:58 100:59 101:60 102:61 103:62 104:63 105:64 106:65 107:66 108:67 109:68 110:68 111:69 112:70 113:71 114:72 115:73 116:74 117:75 118:76 119:77 120:78 121:79 122:79 123:80 124:81 125:82 126:83 127:84 128:85 129:86 130:86 131:87 132:87 133:88 134:89 135:90 136:91 137:92 138:93 139:94 140:94 141:95 142:96 143:97 144:97 145:98 146:99 147:99 148:99 149:100 150:101 151:102 152:103 153:104 154:105 155:106 156:107 157:108 158:109 159:110 160:111 161:112 162:113 163:114 164:115 165:116 166:117 167:117 168:117 169:118 170:119 171:120 172:121 173:121 174:122 175:123 176:124 177:125 178:126 179:127 180:128 181:129 182:130 183:131 184:131 185:132 186:133 187:134 188:135 189:136 190:137 191:138 192:139 193:140 194:141 195:142 196:143 197:143 198:144 199:145 200:146 201:147 202:148 203:149 204:150 205:151 206:151 207:152 208:153 209:154 210:155 211:155 212:156 213:157 214:158 215:159 216:160 217:161 218:162 219:163 220:164 221:165 222:166 223:166 224:167 225:168 226:169 227:170 228:170 229:171 230:172 231:173 232:174 233:175 234:176 235:177 236:177 237:178 238:179 239:180 240:181 241:182 242:183 243:183 244:184 245:185 246:186 247:187 248:188 249:189 250:189 251:190 252:191 253:191 254:192 255:193 256:194 257:195 258:196 259:197 260:197\n","token_is_max_context: 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True\n","input_ids: 101 11840 11934 13410 180 31985 12086 15625 299 10816 10147 108481 66866 15027 24910 10362 117 16396 14369 11213 109327 15237 18097 12552 14421 27258 136 102 12660 13956 30097 20377 117 22862 30355 30906 10447 18755 22313 50816 19923 15049 117 10504 69542 100727 24276 15814 105176 24544 10447 10792 13265 37458 25216 117 11213 43419 43993 16487 12072 299 10816 10147 108481 66866 15027 24910 10362 113 11032 114 117 10149 16449 13956 30097 14441 11755 10601 23087 57182 19114 13848 22380 37458 25216 28298 12393 10331 50816 19923 15049 11213 181 36228 10417 13410 180 31985 12072 71038 16396 14369 20383 10792 60127 23087 12420 12932 117 10149 31811 16396 14369 21820 33493 10792 24544 64922 10447 13956 30097 14441 10432 14808 15237 18097 12460 263 10410 13956 30097 14441 30906 66866 15027 13951 38081 267 10269 10447 16396 14369 119 10504 299 10816 10147 108481 50816 19923 15049 14808 15237 40548 22654 16396 14369 19114 35498 50622 20419 34255 20734 16449 299 10816 10147 108481 10432 16396 61697 117 16396 14369 22389 21820 16325 10432 23375 15605 72228 95271 119 12660 50816 19923 15049 11731 10789 13284 69721 16638 10331 16449 13956 30097 14441 12072 12393 27261 21629 16449 16396 14369 119 12086 16425 40352 52256 117 12660 15049 14294 16549 25993 16948 88636 11182 16449 20419 192 16218 10432 16449 16396 14369 117 10504 12072 19087 10331 15049 16549 41946 117 12522 15509 13148 14393 11182 19013 117 15662 12660 13148 11755 19383 18755 117 11755 176 43021 16948 88636 11182 39920 16449 19087 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 125\n","end_position: 143\n","answer: sử dụng chúng để é ##p vi ##ệt nam ký hiệp định theo lợi í ##ch của trung quốc\n","*** Example ***\n","unique_id: 1000000013\n","example_index: 13\n","doc_span_index: 0\n","tokens: [CLS] trung quốc đã làm gì để làm sai lệ ##ch thông tin văn kiện đến quần chúng nhân dân trung quốc ? [SEP] theo quan điểm của trung quốc ( tài liệu bộ ngoại giao ) , công hàm của thủ tướng nước vi ##ệt nam dân chủ cộng ho ##à phạm văn đồng đương nhiên \" công nhận \" chủ quyền của trung quốc đối với các quần đảo trên biển đông vì trước đó trong số báo ngày 6 tháng 9 báo nhân dân \" đã đăng chi tiết về tuyên bố lãnh hải của chính phủ trung quốc trong đó đường cơ sở để tính lãnh hải bao gồm bờ biển hai quần đảo tây sa và nam sa trên biển đông \" . các tài liệu công bố công khai cho công chúng trung quốc bản dịch tiếng trung đều không dịch đầy đủ nội dung của văn kiện này , thiếu một số nội dung trong đó có đoạn \" tri ##ệt để tôn trọng hải phận 12 hải lý của trung quốc \" . [SEP]\n","token_to_orig_map: 24:0 25:1 26:2 27:3 28:4 29:5 30:6 31:6 32:7 33:8 34:9 35:10 36:10 37:10 38:11 39:12 40:13 41:14 42:15 43:16 44:17 45:17 46:18 47:19 48:20 49:21 50:22 51:22 52:23 53:24 54:25 55:26 56:27 57:28 58:28 59:29 60:29 61:30 62:31 63:32 64:33 65:34 66:35 67:36 68:37 69:38 70:39 71:40 72:41 73:42 74:43 75:44 76:45 77:46 78:47 79:48 80:49 81:50 82:51 83:52 84:53 85:54 86:55 87:56 88:56 89:57 90:58 91:59 92:60 93:61 94:62 95:63 96:64 97:65 98:66 99:67 100:68 101:69 102:70 103:71 104:72 105:73 106:74 107:75 108:76 109:77 110:78 111:79 112:80 113:81 114:82 115:83 116:84 117:85 118:86 119:87 120:88 121:89 122:90 123:91 124:92 125:93 126:93 127:93 128:94 129:95 130:96 131:97 132:98 133:99 134:100 135:101 136:102 137:103 138:104 139:105 140:106 141:107 142:108 143:109 144:110 145:111 146:112 147:113 148:114 149:115 150:116 151:117 152:118 153:119 154:120 155:120 156:121 157:122 158:123 159:124 160:125 161:126 162:127 163:128 164:129 165:130 166:130 167:130 168:131 169:132 170:133 171:134 172:135 173:136 174:137 175:138 176:139 177:140 178:141 179:141 180:141\n","token_is_max_context: 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True\n","input_ids: 101 16396 14369 11213 12984 49309 12460 12984 13410 28935 10269 18030 21629 19923 34018 12002 50489 18097 14694 12486 16396 14369 136 102 13951 12522 15509 10447 16396 14369 113 24083 26314 13848 37458 25216 114 117 12319 64917 10447 18755 22313 12932 13956 30097 14441 12486 16549 30876 13173 10816 50816 19923 15049 70752 17685 107 12319 16638 107 16549 17992 10447 16396 14369 17600 11182 10792 50489 22587 12598 18394 23164 17819 16325 12393 10504 11634 24276 12137 127 11642 130 24276 14694 12486 107 11213 72942 14325 43419 12420 47860 19925 30608 45965 10447 12707 21961 16396 14369 10504 12393 17773 16579 28077 12460 17739 30608 45965 18323 16394 50903 18394 13080 50489 22587 27073 10148 10432 14441 10148 12598 18394 23164 107 119 10792 24083 26314 12319 19925 12319 37949 11257 12319 18097 16396 14369 15194 22654 14537 16396 22389 11755 22654 57773 45496 35498 50622 10447 19923 34018 10789 117 54594 10417 11634 35498 50622 10504 12393 10601 31709 107 15633 30097 12460 50522 22728 45965 50076 10186 45965 17522 10447 16396 14369 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 128\n","end_position: 153\n","answer: các tài liệu công bố công khai cho công chúng trung quốc bản dịch tiếng trung đều không dịch đầy đủ nội dung của văn kiện\n","*** Example ***\n","unique_id: 1000000014\n","example_index: 14\n","doc_span_index: 0\n","tokens: [CLS] quan điểm của trung quốc là gì đối với công hàm của thủ tướng nước vi ##ệt nam dân chủ cộng ho ##à phạm văn đồng ? [SEP] theo quan điểm của trung quốc ( tài liệu bộ ngoại giao ) , công hàm của thủ tướng nước vi ##ệt nam dân chủ cộng ho ##à phạm văn đồng đương nhiên \" công nhận \" chủ quyền của trung quốc đối với các quần đảo trên biển đông vì trước đó trong số báo ngày 6 tháng 9 báo nhân dân \" đã đăng chi tiết về tuyên bố lãnh hải của chính phủ trung quốc trong đó đường cơ sở để tính lãnh hải bao gồm bờ biển hai quần đảo tây sa và nam sa trên biển đông \" . các tài liệu công bố công khai cho công chúng trung quốc bản dịch tiếng trung đều không dịch đầy đủ nội dung của văn kiện này , thiếu một số nội dung trong đó có đoạn \" tri ##ệt để tôn trọng hải phận 12 hải lý của trung quốc \" . [SEP]\n","token_to_orig_map: 29:0 30:1 31:2 32:3 33:4 34:5 35:6 36:6 37:7 38:8 39:9 40:10 41:10 42:10 43:11 44:12 45:13 46:14 47:15 48:16 49:17 50:17 51:18 52:19 53:20 54:21 55:22 56:22 57:23 58:24 59:25 60:26 61:27 62:28 63:28 64:29 65:29 66:30 67:31 68:32 69:33 70:34 71:35 72:36 73:37 74:38 75:39 76:40 77:41 78:42 79:43 80:44 81:45 82:46 83:47 84:48 85:49 86:50 87:51 88:52 89:53 90:54 91:55 92:56 93:56 94:57 95:58 96:59 97:60 98:61 99:62 100:63 101:64 102:65 103:66 104:67 105:68 106:69 107:70 108:71 109:72 110:73 111:74 112:75 113:76 114:77 115:78 116:79 117:80 118:81 119:82 120:83 121:84 122:85 123:86 124:87 125:88 126:89 127:90 128:91 129:92 130:93 131:93 132:93 133:94 134:95 135:96 136:97 137:98 138:99 139:100 140:101 141:102 142:103 143:104 144:105 145:106 146:107 147:108 148:109 149:110 150:111 151:112 152:113 153:114 154:115 155:116 156:117 157:118 158:119 159:120 160:120 161:121 162:122 163:123 164:124 165:125 166:126 167:127 168:128 169:129 170:130 171:130 172:130 173:131 174:132 175:133 176:134 177:135 178:136 179:137 180:138 181:139 182:140 183:141 184:141 185:141\n","token_is_max_context: 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True\n","input_ids: 101 12522 15509 10447 16396 14369 10331 49309 17600 11182 12319 64917 10447 18755 22313 12932 13956 30097 14441 12486 16549 30876 13173 10816 50816 19923 15049 136 102 13951 12522 15509 10447 16396 14369 113 24083 26314 13848 37458 25216 114 117 12319 64917 10447 18755 22313 12932 13956 30097 14441 12486 16549 30876 13173 10816 50816 19923 15049 70752 17685 107 12319 16638 107 16549 17992 10447 16396 14369 17600 11182 10792 50489 22587 12598 18394 23164 17819 16325 12393 10504 11634 24276 12137 127 11642 130 24276 14694 12486 107 11213 72942 14325 43419 12420 47860 19925 30608 45965 10447 12707 21961 16396 14369 10504 12393 17773 16579 28077 12460 17739 30608 45965 18323 16394 50903 18394 13080 50489 22587 27073 10148 10432 14441 10148 12598 18394 23164 107 119 10792 24083 26314 12319 19925 12319 37949 11257 12319 18097 16396 14369 15194 22654 14537 16396 22389 11755 22654 57773 45496 35498 50622 10447 19923 34018 10789 117 54594 10417 11634 35498 50622 10504 12393 10601 31709 107 15633 30097 12460 50522 22728 45965 50076 10186 45965 17522 10447 16396 14369 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 60\n","end_position: 78\n","answer: đương nhiên \" công nhận \" chủ quyền của trung quốc đối với các quần đảo trên biển đông\n","*** Example ***\n","unique_id: 1000000015\n","example_index: 15\n","doc_span_index: 0\n","tokens: [CLS] nội dung nào được đề cập trong báo nhân dân ngày 6 tháng 9 ? [SEP] theo quan điểm của trung quốc ( tài liệu bộ ngoại giao ) , công hàm của thủ tướng nước vi ##ệt nam dân chủ cộng ho ##à phạm văn đồng đương nhiên \" công nhận \" chủ quyền của trung quốc đối với các quần đảo trên biển đông vì trước đó trong số báo ngày 6 tháng 9 báo nhân dân \" đã đăng chi tiết về tuyên bố lãnh hải của chính phủ trung quốc trong đó đường cơ sở để tính lãnh hải bao gồm bờ biển hai quần đảo tây sa và nam sa trên biển đông \" . các tài liệu công bố công khai cho công chúng trung quốc bản dịch tiếng trung đều không dịch đầy đủ nội dung của văn kiện này , thiếu một số nội dung trong đó có đoạn \" tri ##ệt để tôn trọng hải phận 12 hải lý của trung quốc \" . [SEP]\n","token_to_orig_map: 17:0 18:1 19:2 20:3 21:4 22:5 23:6 24:6 25:7 26:8 27:9 28:10 29:10 30:10 31:11 32:12 33:13 34:14 35:15 36:16 37:17 38:17 39:18 40:19 41:20 42:21 43:22 44:22 45:23 46:24 47:25 48:26 49:27 50:28 51:28 52:29 53:29 54:30 55:31 56:32 57:33 58:34 59:35 60:36 61:37 62:38 63:39 64:40 65:41 66:42 67:43 68:44 69:45 70:46 71:47 72:48 73:49 74:50 75:51 76:52 77:53 78:54 79:55 80:56 81:56 82:57 83:58 84:59 85:60 86:61 87:62 88:63 89:64 90:65 91:66 92:67 93:68 94:69 95:70 96:71 97:72 98:73 99:74 100:75 101:76 102:77 103:78 104:79 105:80 106:81 107:82 108:83 109:84 110:85 111:86 112:87 113:88 114:89 115:90 116:91 117:92 118:93 119:93 120:93 121:94 122:95 123:96 124:97 125:98 126:99 127:100 128:101 129:102 130:103 131:104 132:105 133:106 134:107 135:108 136:109 137:110 138:111 139:112 140:113 141:114 142:115 143:116 144:117 145:118 146:119 147:120 148:120 149:121 150:122 151:123 152:124 153:125 154:126 155:127 156:128 157:129 158:130 159:130 160:130 161:131 162:132 163:133 164:134 165:135 166:136 167:137 168:138 169:139 170:140 171:141 172:141 173:141\n","token_is_max_context: 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True\n","input_ids: 101 35498 50622 27258 10476 20192 41472 10504 24276 14694 12486 12137 127 11642 130 136 102 13951 12522 15509 10447 16396 14369 113 24083 26314 13848 37458 25216 114 117 12319 64917 10447 18755 22313 12932 13956 30097 14441 12486 16549 30876 13173 10816 50816 19923 15049 70752 17685 107 12319 16638 107 16549 17992 10447 16396 14369 17600 11182 10792 50489 22587 12598 18394 23164 17819 16325 12393 10504 11634 24276 12137 127 11642 130 24276 14694 12486 107 11213 72942 14325 43419 12420 47860 19925 30608 45965 10447 12707 21961 16396 14369 10504 12393 17773 16579 28077 12460 17739 30608 45965 18323 16394 50903 18394 13080 50489 22587 27073 10148 10432 14441 10148 12598 18394 23164 107 119 10792 24083 26314 12319 19925 12319 37949 11257 12319 18097 16396 14369 15194 22654 14537 16396 22389 11755 22654 57773 45496 35498 50622 10447 19923 34018 10789 117 54594 10417 11634 35498 50622 10504 12393 10601 31709 107 15633 30097 12460 50522 22728 45965 50076 10186 45965 17522 10447 16396 14369 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 86\n","end_position: 118\n","answer: tuyên bố lãnh hải của chính phủ trung quốc trong đó đường cơ sở để tính lãnh hải bao gồm bờ biển hai quần đảo tây sa và nam sa trên biển đông\n","*** Example ***\n","unique_id: 1000000016\n","example_index: 16\n","doc_span_index: 0\n","tokens: [CLS] lập trường của vi ##ệt nam trong việc giải quyết các vấn đề tranh chấp quần đảo hoàng sa và trường sa là gì ? [SEP] theo tuyên bố của bộ ngoại giao nước ch ##x ##h ##c ##n vi ##ệt nam về quần đảo hoàng sa và trường sa ngày 7 tháng 8 năm 1979 thì : sự diễn giải của trung quốc về bản công hàm ngày 14 tháng 9 năm 1958 của thủ tướng nước vi ##ệt nam dân chủ cộng ho ##à như một sự công nhận chủ quyền của phía trung quốc trên các quần đảo là một sự xuyên t ##ạc trắng trợ ##n khi tinh thần và ý nghĩa của bản công hàm chỉ có ý định công nhận giới hạn 12 hải lý của lãnh hải trung quốc . ( tức là khoảng 22 km , mà không cho biết đường cơ sở để tính chủ quyền 12 hải lý có bao gồm đường bờ biển hai quần đảo đó hay không ) . văn bản này không đề cập đến hai quần đảo ( chỉ là cơ sở trung quốc tính hải phận và không thuộc hải phận ) . vi ##ệt nam k ##h ##ẳng định lại chủ quyền đối với 2 quần đảo này , n ##hắc lại lập trường của vi ##ệt nam về việc giải quyết sự tranh chấp về 2 quần đảo giữa hai nước bằng thương lượng ho ##à bình . [SEP]\n","token_to_orig_map: 27:0 28:1 29:2 30:3 31:4 32:5 33:6 34:7 35:8 36:8 37:8 38:8 39:8 40:9 41:9 42:10 43:11 44:12 45:13 46:14 47:15 48:16 49:17 50:18 51:19 52:20 53:21 54:22 55:23 56:24 57:25 58:25 59:26 60:27 61:28 62:29 63:30 64:31 65:32 66:33 67:34 68:35 69:36 70:37 71:38 72:39 73:40 74:41 75:42 76:43 77:44 78:45 79:46 80:46 81:47 82:48 83:49 84:50 85:51 86:51 87:52 88:53 89:54 90:55 91:56 92:57 93:58 94:59 95:60 96:61 97:62 98:63 99:64 100:65 101:66 102:67 103:68 104:69 105:70 106:71 107:71 108:72 109:73 110:73 111:74 112:75 113:76 114:77 115:78 116:79 117:80 118:81 119:82 120:83 121:84 122:85 123:86 124:87 125:88 126:89 127:90 128:91 129:92 130:93 131:94 132:95 133:96 134:97 135:98 136:99 137:99 138:100 139:100 140:101 141:102 142:103 143:104 144:104 145:105 146:106 147:107 148:108 149:109 150:110 151:111 152:112 153:113 154:114 155:115 156:116 157:117 158:118 159:119 160:120 161:121 162:122 163:123 164:124 165:125 166:126 167:127 168:128 169:129 170:130 171:130 172:130 173:131 174:132 175:133 176:134 177:135 178:136 179:137 180:138 181:139 182:140 183:141 184:141 185:142 186:143 187:144 188:145 189:146 190:147 191:148 192:149 193:150 194:151 195:152 196:153 197:154 198:154 199:154 200:155 201:155 202:156 203:157 204:157 205:157 206:158 207:159 208:160 209:161 210:162 211:163 212:164 213:165 214:166 215:167 216:167 217:168 218:168 219:169 220:170 221:171 222:172 223:173 224:173 225:174 226:175 227:176 228:177 229:178 230:179 231:180 232:181 233:182 234:183 235:184 236:185 237:186 238:187 239:188 240:189 241:190 242:191 243:192 244:192 245:193 246:193\n","token_is_max_context: 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True\n","input_ids: 101 15707 16188 10447 13956 30097 14441 10504 14331 16658 27016 10792 35560 20192 19383 50645 50489 22587 25730 10148 10432 16188 10148 10331 49309 136 102 13951 47860 19925 10447 13848 37458 25216 12932 18643 10686 10237 10350 10115 13956 30097 14441 12420 50489 22587 25730 10148 10432 16188 10148 12137 128 11642 129 10558 10675 18004 131 12636 17300 16658 10447 16396 14369 12420 15194 12319 64917 12137 10247 11642 130 10558 10947 10447 18755 22313 12932 13956 30097 14441 12486 16549 30876 13173 10816 12552 10417 12636 12319 16638 16549 17992 10447 15564 16396 14369 12598 10792 50489 22587 10331 10417 12636 69277 188 31607 52666 30627 10115 12072 24009 26082 10432 283 20792 10447 15194 12319 64917 14294 10601 283 15027 12319 16638 15814 33756 10186 45965 17522 10447 30608 45965 16396 14369 119 113 39321 10331 18706 10306 10204 117 15542 11755 11257 21820 17773 16579 28077 12460 17739 16549 17992 10186 45965 17522 10601 18323 16394 17773 50903 18394 13080 50489 22587 12393 13605 11755 114 119 19923 15194 10789 11755 20192 41472 12002 13080 50489 22587 113 14294 10331 16579 28077 16396 14369 17739 45965 50076 10432 11755 12005 45965 50076 114 119 13956 30097 14441 179 10237 73882 15027 13148 16549 17992 17600 11182 123 50489 22587 10789 117 182 79996 13148 15707 16188 10447 13956 30097 14441 12420 14331 16658 27016 12636 19383 50645 12420 123 50489 22587 20734 13080 12932 15991 26802 15876 13173 10816 20438 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 241\n","end_position: 245\n","answer: thương lượng ho ##à bình\n","*** Example ***\n","unique_id: 1000000017\n","example_index: 17\n","doc_span_index: 0\n","tokens: [CLS] nội dung chủ yếu trong tuyên bố của bộ ngoại giao nước ch ##x ##h ##c ##n vi ##ệt nam về quần đảo hoàng sa và trường sa ngày 7 tháng 8 năm 1979 là gì ? [SEP] theo tuyên bố của bộ ngoại giao nước ch ##x ##h ##c ##n vi ##ệt nam về quần đảo hoàng sa và trường sa ngày 7 tháng 8 năm 1979 thì : sự diễn giải của trung quốc về bản công hàm ngày 14 tháng 9 năm 1958 của thủ tướng nước vi ##ệt nam dân chủ cộng ho ##à như một sự công nhận chủ quyền của phía trung quốc trên các quần đảo là một sự xuyên t ##ạc trắng trợ ##n khi tinh thần và ý nghĩa của bản công hàm chỉ có ý định công nhận giới hạn 12 hải lý của lãnh hải trung quốc . ( tức là khoảng 22 km , mà không cho biết đường cơ sở để tính chủ quyền 12 hải lý có bao gồm đường bờ biển hai quần đảo đó hay không ) . văn bản này không đề cập đến hai quần đảo ( chỉ là cơ sở trung quốc tính hải phận và không thuộc hải phận ) . vi ##ệt nam k ##h ##ẳng định lại chủ quyền đối với 2 quần đảo này , n ##hắc lại lập trường của vi ##ệt nam về việc giải quyết sự tranh chấp về 2 quần đảo giữa hai nước bằng thương lượng ho ##à bình . [SEP]\n","token_to_orig_map: 39:0 40:1 41:2 42:3 43:4 44:5 45:6 46:7 47:8 48:8 49:8 50:8 51:8 52:9 53:9 54:10 55:11 56:12 57:13 58:14 59:15 60:16 61:17 62:18 63:19 64:20 65:21 66:22 67:23 68:24 69:25 70:25 71:26 72:27 73:28 74:29 75:30 76:31 77:32 78:33 79:34 80:35 81:36 82:37 83:38 84:39 85:40 86:41 87:42 88:43 89:44 90:45 91:46 92:46 93:47 94:48 95:49 96:50 97:51 98:51 99:52 100:53 101:54 102:55 103:56 104:57 105:58 106:59 107:60 108:61 109:62 110:63 111:64 112:65 113:66 114:67 115:68 116:69 117:70 118:71 119:71 120:72 121:73 122:73 123:74 124:75 125:76 126:77 127:78 128:79 129:80 130:81 131:82 132:83 133:84 134:85 135:86 136:87 137:88 138:89 139:90 140:91 141:92 142:93 143:94 144:95 145:96 146:97 147:98 148:99 149:99 150:100 151:100 152:101 153:102 154:103 155:104 156:104 157:105 158:106 159:107 160:108 161:109 162:110 163:111 164:112 165:113 166:114 167:115 168:116 169:117 170:118 171:119 172:120 173:121 174:122 175:123 176:124 177:125 178:126 179:127 180:128 181:129 182:130 183:130 184:130 185:131 186:132 187:133 188:134 189:135 190:136 191:137 192:138 193:139 194:140 195:141 196:141 197:142 198:143 199:144 200:145 201:146 202:147 203:148 204:149 205:150 206:151 207:152 208:153 209:154 210:154 211:154 212:155 213:155 214:156 215:157 216:157 217:157 218:158 219:159 220:160 221:161 222:162 223:163 224:164 225:165 226:166 227:167 228:167 229:168 230:168 231:169 232:170 233:171 234:172 235:173 236:173 237:174 238:175 239:176 240:177 241:178 242:179 243:180 244:181 245:182 246:183 247:184 248:185 249:186 250:187 251:188 252:189 253:190 254:191 255:192 256:192 257:193 258:193\n","token_is_max_context: 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True\n","input_ids: 101 35498 50622 16549 25993 10504 47860 19925 10447 13848 37458 25216 12932 18643 10686 10237 10350 10115 13956 30097 14441 12420 50489 22587 25730 10148 10432 16188 10148 12137 128 11642 129 10558 10675 10331 49309 136 102 13951 47860 19925 10447 13848 37458 25216 12932 18643 10686 10237 10350 10115 13956 30097 14441 12420 50489 22587 25730 10148 10432 16188 10148 12137 128 11642 129 10558 10675 18004 131 12636 17300 16658 10447 16396 14369 12420 15194 12319 64917 12137 10247 11642 130 10558 10947 10447 18755 22313 12932 13956 30097 14441 12486 16549 30876 13173 10816 12552 10417 12636 12319 16638 16549 17992 10447 15564 16396 14369 12598 10792 50489 22587 10331 10417 12636 69277 188 31607 52666 30627 10115 12072 24009 26082 10432 283 20792 10447 15194 12319 64917 14294 10601 283 15027 12319 16638 15814 33756 10186 45965 17522 10447 30608 45965 16396 14369 119 113 39321 10331 18706 10306 10204 117 15542 11755 11257 21820 17773 16579 28077 12460 17739 16549 17992 10186 45965 17522 10601 18323 16394 17773 50903 18394 13080 50489 22587 12393 13605 11755 114 119 19923 15194 10789 11755 20192 41472 12002 13080 50489 22587 113 14294 10331 16579 28077 16396 14369 17739 45965 50076 10432 11755 12005 45965 50076 114 119 13956 30097 14441 179 10237 73882 15027 13148 16549 17992 17600 11182 123 50489 22587 10789 117 182 79996 13148 15707 16188 10447 13956 30097 14441 12420 14331 16658 27016 12636 19383 50645 12420 123 50489 22587 20734 13080 12932 15991 26802 15876 13173 10816 20438 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 212\n","end_position: 257\n","answer: vi ##ệt nam k ##h ##ẳng định lại chủ quyền đối với 2 quần đảo này , n ##hắc lại lập trường của vi ##ệt nam về việc giải quyết sự tranh chấp về 2 quần đảo giữa hai nước bằng thương lượng ho ##à bình\n","*** Example ***\n","unique_id: 1000000018\n","example_index: 18\n","doc_span_index: 0\n","tokens: [CLS] lời diễn giải về bản công hàm ngày 14 tháng 9 năm 1958 được bộ ngoại giao nước ch ##x ##h ##c ##n vi ##ệt nam nhìn nhận như thế nào ? [SEP] theo tuyên bố của bộ ngoại giao nước ch ##x ##h ##c ##n vi ##ệt nam về quần đảo hoàng sa và trường sa ngày 7 tháng 8 năm 1979 thì : sự diễn giải của trung quốc về bản công hàm ngày 14 tháng 9 năm 1958 của thủ tướng nước vi ##ệt nam dân chủ cộng ho ##à như một sự công nhận chủ quyền của phía trung quốc trên các quần đảo là một sự xuyên t ##ạc trắng trợ ##n khi tinh thần và ý nghĩa của bản công hàm chỉ có ý định công nhận giới hạn 12 hải lý của lãnh hải trung quốc . ( tức là khoảng 22 km , mà không cho biết đường cơ sở để tính chủ quyền 12 hải lý có bao gồm đường bờ biển hai quần đảo đó hay không ) . văn bản này không đề cập đến hai quần đảo ( chỉ là cơ sở trung quốc tính hải phận và không thuộc hải phận ) . vi ##ệt nam k ##h ##ẳng định lại chủ quyền đối với 2 quần đảo này , n ##hắc lại lập trường của vi ##ệt nam về việc giải quyết sự tranh chấp về 2 quần đảo giữa hai nước bằng thương lượng ho ##à bình . [SEP]\n","token_to_orig_map: 34:0 35:1 36:2 37:3 38:4 39:5 40:6 41:7 42:8 43:8 44:8 45:8 46:8 47:9 48:9 49:10 50:11 51:12 52:13 53:14 54:15 55:16 56:17 57:18 58:19 59:20 60:21 61:22 62:23 63:24 64:25 65:25 66:26 67:27 68:28 69:29 70:30 71:31 72:32 73:33 74:34 75:35 76:36 77:37 78:38 79:39 80:40 81:41 82:42 83:43 84:44 85:45 86:46 87:46 88:47 89:48 90:49 91:50 92:51 93:51 94:52 95:53 96:54 97:55 98:56 99:57 100:58 101:59 102:60 103:61 104:62 105:63 106:64 107:65 108:66 109:67 110:68 111:69 112:70 113:71 114:71 115:72 116:73 117:73 118:74 119:75 120:76 121:77 122:78 123:79 124:80 125:81 126:82 127:83 128:84 129:85 130:86 131:87 132:88 133:89 134:90 135:91 136:92 137:93 138:94 139:95 140:96 141:97 142:98 143:99 144:99 145:100 146:100 147:101 148:102 149:103 150:104 151:104 152:105 153:106 154:107 155:108 156:109 157:110 158:111 159:112 160:113 161:114 162:115 163:116 164:117 165:118 166:119 167:120 168:121 169:122 170:123 171:124 172:125 173:126 174:127 175:128 176:129 177:130 178:130 179:130 180:131 181:132 182:133 183:134 184:135 185:136 186:137 187:138 188:139 189:140 190:141 191:141 192:142 193:143 194:144 195:145 196:146 197:147 198:148 199:149 200:150 201:151 202:152 203:153 204:154 205:154 206:154 207:155 208:155 209:156 210:157 211:157 212:157 213:158 214:159 215:160 216:161 217:162 218:163 219:164 220:165 221:166 222:167 223:167 224:168 225:168 226:169 227:170 228:171 229:172 230:173 231:173 232:174 233:175 234:176 235:177 236:178 237:179 238:180 239:181 240:182 241:183 242:184 243:185 244:186 245:187 246:188 247:189 248:190 249:191 250:192 251:192 252:193 253:193\n","token_is_max_context: 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True\n","input_ids: 101 34619 17300 16658 12420 15194 12319 64917 12137 10247 11642 130 10558 10947 10476 13848 37458 25216 12932 18643 10686 10237 10350 10115 13956 30097 14441 54762 16638 12552 14421 27258 136 102 13951 47860 19925 10447 13848 37458 25216 12932 18643 10686 10237 10350 10115 13956 30097 14441 12420 50489 22587 25730 10148 10432 16188 10148 12137 128 11642 129 10558 10675 18004 131 12636 17300 16658 10447 16396 14369 12420 15194 12319 64917 12137 10247 11642 130 10558 10947 10447 18755 22313 12932 13956 30097 14441 12486 16549 30876 13173 10816 12552 10417 12636 12319 16638 16549 17992 10447 15564 16396 14369 12598 10792 50489 22587 10331 10417 12636 69277 188 31607 52666 30627 10115 12072 24009 26082 10432 283 20792 10447 15194 12319 64917 14294 10601 283 15027 12319 16638 15814 33756 10186 45965 17522 10447 30608 45965 16396 14369 119 113 39321 10331 18706 10306 10204 117 15542 11755 11257 21820 17773 16579 28077 12460 17739 16549 17992 10186 45965 17522 10601 18323 16394 17773 50903 18394 13080 50489 22587 12393 13605 11755 114 119 19923 15194 10789 11755 20192 41472 12002 13080 50489 22587 113 14294 10331 16579 28077 16396 14369 17739 45965 50076 10432 11755 12005 45965 50076 114 119 13956 30097 14441 179 10237 73882 15027 13148 16549 17992 17600 11182 123 50489 22587 10789 117 182 79996 13148 15707 16188 10447 13956 30097 14441 12420 14331 16658 27016 12636 19383 50645 12420 123 50489 22587 20734 13080 12932 15991 26802 15876 13173 10816 20438 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 110\n","end_position: 117\n","answer: một sự xuyên t ##ạc trắng trợ ##n\n","*** Example ***\n","unique_id: 1000000019\n","example_index: 19\n","doc_span_index: 0\n","tokens: [CLS] tại sao lòng tin của nhân dân vào đảng lại giảm sú ##t ? [SEP] điều chủ yếu phải n ##h ##ấn mạnh theo tinh thần trên là nhiều người có chức , có quyền trong hệ thống tổ chức của đảng , nhà nước , các đoàn thể quần chúng h ##ư h ##ỏng quá , th ##o ##ái hóa biến chất , chạy theo chức quyền , tiền và danh lợi , những người ấy đang làm cho một bộ phận không nhỏ trong nhân dân ta ngày càng giảm lòng tin vào đảng ta , đưa đến tình hình nguy kịch , không thể coi thường , là sự hội nhập của \" bốn nguy cơ \" tác động lẫn nhau và phá ta , có thể đưa đến sự mất còn của chế độ và sự nghiệp cách mạng của chúng ta . [SEP]\n","token_to_orig_map: 16:0 17:1 18:2 19:3 20:4 21:4 22:4 23:5 24:6 25:7 26:8 27:9 28:10 29:11 30:12 31:13 32:14 33:14 34:15 35:16 36:17 37:18 38:19 39:20 40:21 41:22 42:23 43:23 44:24 45:25 46:25 47:26 48:27 49:28 50:29 51:30 52:31 53:31 54:32 55:32 56:33 57:33 58:34 59:34 60:34 61:35 62:36 63:37 64:37 65:38 66:39 67:40 68:41 69:41 70:42 71:43 72:44 73:45 74:45 75:46 76:47 77:48 78:49 79:50 80:51 81:52 82:53 83:54 84:55 85:56 86:57 87:58 88:59 89:60 90:61 91:62 92:63 93:64 94:65 95:66 96:67 97:68 98:68 99:69 100:70 101:71 102:72 103:73 104:74 105:74 106:75 107:76 108:77 109:78 110:78 111:79 112:80 113:81 114:82 115:83 116:84 117:84 118:85 119:86 120:86 121:87 122:88 123:89 124:90 125:91 126:92 127:93 128:93 129:94 130:95 131:96 132:97 133:98 134:99 135:100 136:101 137:102 138:103 139:104 140:105 141:106 142:107 143:108 144:109 145:110 146:111 147:111\n","token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True\n","input_ids: 101 12086 36993 53970 21629 10447 14694 12486 11603 64630 13148 39140 17024 10123 136 102 16391 16549 25993 15723 182 10237 33448 29172 13951 24009 26082 12598 10331 13710 11027 10601 16244 117 10601 17992 10504 18310 16805 21217 16244 10447 64630 117 13265 12932 117 10792 16449 12334 50489 18097 176 28776 176 56876 27261 117 77586 10133 26310 20105 23719 21913 117 38135 13951 16244 17992 117 28851 10432 22254 38081 117 11934 11027 52256 21080 12984 11257 10417 13848 50076 11755 23034 10504 14694 12486 11057 12137 57255 39140 53970 21629 11603 64630 11057 117 25576 12002 23403 15054 75578 61117 117 11755 12334 26434 16591 117 10331 12636 16425 26785 10447 107 52702 75578 16579 107 17976 13254 91281 20875 10432 32015 11057 117 10601 12334 25576 12002 12636 28022 14674 10447 21634 14776 10432 12636 20608 15605 34855 10447 18097 11057 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","start_position: 29\n","end_position: 73\n","answer: nhiều người có chức , có quyền trong hệ thống tổ chức của đảng , nhà nước , các đoàn thể quần chúng h ##ư h ##ỏng quá , th ##o ##ái hóa biến chất , chạy theo chức quyền , tiền và danh lợi\n","Saving features into cached file %s ./ViQuAD1.0/cached_train_bert-base-multilingual-cased_384\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["***** Running training *****\n","  Num examples = %d 20342\n","  Num Epochs = %d 3\n","  Instantaneous batch size per GPU = %d 4\n","  Total train batch size (w. parallel, distributed & accumulation) = %d 4\n","  Gradient Accumulation steps = %d 1\n","  Total optimization steps = %d 15258\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-1000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-2000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-3000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-4000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-5000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-6000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-7000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-8000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-9000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-10000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-11000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-12000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-13000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-14000\n","Saving model checkpoint to %s models/bert-finetuned-vinews1/checkpoint-15000\n"," global_step = %s, average loss = %s 15258 1.1415766773333775\n","Saving model checkpoint to %s models/bert-finetuned-vinews1\n"]},{"output_type":"error","ename":"TypeError","evalue":"Object of type method is not JSON serializable","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-23-990a629dca3e>\u001b[0m in \u001b[0;36m<cell line: 16>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mmodel_to_save\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'module'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmodel_to_save\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output_dir\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36msave_pretrained\u001b[0;34m(self, save_directory, legacy_format, filename_prefix, push_to_hub, **kwargs)\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2425\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_config_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2426\u001b[0;31m             \u001b[0mout_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_ascii\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2427\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2428\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizer config file saved in {tokenizer_config_file}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/__init__.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    236\u001b[0m         \u001b[0mcheck_circular\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_circular\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0mseparators\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseparators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 238\u001b[0;31m         **kw).encode(obj)\n\u001b[0m\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_one_shot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode_dict\u001b[0;34m(dct, _current_indent_level)\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m                     \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 405\u001b[0;31m                 \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mchunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    406\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnewline_indent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0m_current_indent_level\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36m_iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    436\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Circular reference detected\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0mmarkers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmarkerid\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m             \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m_iterencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_current_indent_level\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmarkers\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.10/json/encoder.py\u001b[0m in \u001b[0;36mdefault\u001b[0;34m(self, o)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \"\"\"\n\u001b[0;32m--> 179\u001b[0;31m         raise TypeError(f'Object of type {o.__class__.__name__} '\n\u001b[0m\u001b[1;32m    180\u001b[0m                         f'is not JSON serializable')\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Object of type method is not JSON serializable"]}]},{"cell_type":"code","source":["# Evaluation - we can ask to evaluate all the checkpoints (sub-directories) in a directory\n","results = {}\n","if args[\"do_eval\"] and args[\"local_rank\"] in [-1, 0]:\n","    checkpoints = [args[\"output_dir\"]]\n","    if args[\"eval_all_checkpoints\"]:\n","        checkpoints = list(os.path.dirname(c) for c in sorted(glob.glob(args[\"output_dir\"] + '/**/' + WEIGHTS_NAME, recursive=True)))\n","\n","    print(\"Evaluate the following checkpoints: %s\", checkpoints)\n","\n","    for checkpoint in checkpoints:\n","        # Reload the model\n","        global_step = checkpoint.split('-')[-1] if len(checkpoints) > 1 else \"\"\n","        model = model_class.from_pretrained(checkpoint)\n","        model.to(device)\n","\n","        # Evaluate\n","        result = evaluate(args, model, tokenizer, prefix=global_step)\n","\n","        result = dict((k + ('_{}'.format(global_step) if global_step else ''), v) for k, v in result.items())\n","        results.update(result)\n","\n","print(\"Results: {}\".format(results))"],"metadata":{"id":"kVwX7mGNJEG7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_f1 = 0.\n","best_checkpoint = None\n","\n","for key in results:\n","    if 'f1' in key and results[key] > max_f1:\n","        max_f1 = results[key]\n","        best_checkpoint = key\n","\n","print(best_checkpoint, max_f1)"],"metadata":{"id":"qauTXWvWm5yc"},"execution_count":null,"outputs":[]}]}